{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# CSE 253: Programming Assignment 1\n",
    "# Code snippet by Jenny Hamer\n",
    "# Winter 2019\n",
    "################################################################################\n",
    "# We've provided you with the dataset in CAFE.tar.gz. To uncompress, use:\n",
    "# tar -xzvf CAFE.tar.gz\n",
    "################################################################################\n",
    "# To install PIL, refer to the instructions for your system:\n",
    "# https://pillow.readthedocs.io/en/5.2.x/installation.html\n",
    "################################################################################\n",
    "# If you don't have NumPy installed, please use the instructions here:\n",
    "# https://scipy.org/install.html\n",
    "################################################################################\n",
    "\n",
    "from os import listdir\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from scipy import linalg as la\n",
    "\n",
    "# The relative path to your CAFE-Gamma dataset\n",
    "data_dir = \"./CAFE/\"\n",
    "\n",
    "# Dictionary of semantic \"label\" to emotions\n",
    "emotion_dict = {\"h\": \"happy\", \"ht\": \"happy with teeth\", \"m\": \"maudlin\",\n",
    "\t\"s\": \"surprise\", \"f\": \"fear\", \"a\": \"anger\", \"d\": \"disgust\", \"n\": \"neutral\"}\n",
    "\n",
    "\n",
    "def load_data(data_dir=\"./CAFE/\"):\n",
    "    \"\"\" Load all PGM images stored in your data directory into a list of NumPy\n",
    "    arrays with a list of corresponding labels.\n",
    "\n",
    "    Args:\n",
    "        data_dir: The relative filepath to the CAFE dataset.\n",
    "    Returns:\n",
    "        images: A list containing every image in CAFE as an array.\n",
    "        labels: A list of the corresponding labels (filenames) for each image.\n",
    "    \"\"\"\n",
    "    # Get the list of image file names\n",
    "    all_files = listdir(data_dir)\n",
    "\n",
    "    # Store the images as arrays and their labels in two lists\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    for file in all_files:\n",
    "    # Load in the files as PIL images and convert to NumPy arrays\n",
    "        if file.find('_ht') == -1 and file.find('_n') == -1:\n",
    "            img = Image.open(data_dir + file)\n",
    "            images.append(np.array(img))\n",
    "            labels.append(file)\n",
    "\n",
    "    print(\"Total number of images:\", len(images), \"and labels:\", len(labels))\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### TODO  DEVIDE Eigen VEctore to EigenCVALUE\n",
    "\n",
    "def PCA(data, dims_rescaled_data=2):\n",
    "    \"\"\"\n",
    "    returns: data transformed in 2 dims/columns + regenerated original data\n",
    "    pass in: data as 2D NumPy array\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # mean center the data\n",
    "    mean = data.mean(axis=1)\n",
    "    std = data.std(axis=1)\n",
    "    data = data-mean\n",
    "    data = data_T.transpose()\n",
    "\n",
    "    # calculate the covariance matrix\n",
    "    print(data_T.shape)\n",
    "    print(data.shape)\n",
    "    R = np.dot(data_T,data)\n",
    "    evals, evecs = LA.eigh(R)\n",
    "    # sort eigenvalue in decreasing order\n",
    "    idx = NP.argsort(evals)[::-1]\n",
    "    vect = NP.dot(data,evecs)\n",
    "    evecs = vect[:,idx]\n",
    "    # print(evecs.shape)\n",
    "    evals = evals[idx]\n",
    "    print(evals)\n",
    "    evecs = evecs/np.sqrt(evals) \n",
    "    evecs = evecs[:,:dims_rescaled_data]\n",
    "    ##vect = vect-vect.mean(axis = 0)\n",
    "    #vect = vect[:,:dims_rescaled_data]\n",
    "    result = NP.dot(data_T,evecs[:,0])\n",
    "    result = result/np.linalg.norm(evecs[:,0])\n",
    "    #result = result - result.mean(axis =0) \n",
    "    \"\"\"\n",
    "    AA = data.T\n",
    "    mean = AA.mean(axis=1)\n",
    "    std = AA.std(axis=1)\n",
    "    A = ((AA.T-mean)).T\n",
    "    A_T = A.T\n",
    "    n = A_T.shape[0] \n",
    "    sm = np.matmul((1/n)*A_T,A)\n",
    "    evals,ev = la.eigh(sm)\n",
    "    idx = np.argsort(evals)[::-1]\n",
    "    evals = evals[idx]\n",
    "    ev = ev[:,idx] ## 48*48\n",
    "    ##Av = np.matmul(A,ev)\n",
    "    ##evct = Av[:,:dims_rescaled_data]\n",
    "    ## why do these two lines not work ????? sigh\n",
    "    result = []\n",
    "    ev_r = []\n",
    "    for i in range(dims_rescaled_data):\n",
    "        ev_i = np.matmul(A,ev[:,i])/(np.linalg.norm(np.matmul(A,ev[:,i]))*np.sqrt(evals[0]))\n",
    "        ev_r.append(ev_i)\n",
    "        result.append(np.matmul(A_T,ev_i))\n",
    "    result = np.array(result).T\n",
    "    \n",
    "    return result,np.array(ev_r),evals,mean,std\n",
    "\n",
    "def test_PCA(data, dims_rescaled_data=2):\n",
    "    '''\n",
    "    test by attempting to recover original data array from\n",
    "    the eigenvectors of its covariance matrix & comparing that\n",
    "    'recovered' array with the original data\n",
    "    '''\n",
    "    _ , _ , eigenvectors = PCA(data, dim_rescaled_data=2)\n",
    "    data_recovered = NP.dot(eigenvectors, m).T\n",
    "    data_recovered += data_recovered.mean(axis=0)\n",
    "    assert NP.allclose(data, data_recovered)\n",
    "\n",
    "\n",
    "def plot_pca(data):\n",
    "    from matplotlib import pyplot as MPL\n",
    "    clr1 =  '#2026B2'\n",
    "    fig = MPL.figure()\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    data_resc, data_orig = PCA(data)\n",
    "    ax1.plot(data_resc[:, 0], data_resc[:, 1], '.', mfc=clr1, mec=clr1)\n",
    "    MPL.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dims_rescaled_data=47\n",
    "AA = im_re.T\n",
    "mean = AA.mean(axis=1)\n",
    "std = AA.std(axis=1)\n",
    "A = ((AA.T-mean)).T\n",
    "A_T = A.T\n",
    "sm = np.matmul((1/48)*A_T,A)\n",
    "evals,ev = la.eigh(sm)\n",
    "idx = np.argsort(evals)[::-1]\n",
    "evals = evals[idx]\n",
    "ev = ev[:,idx]\n",
    "##Av = np.matmul(A,ev)\n",
    "##evct = Av[:,:dims_rescaled_data]\n",
    "## why do these two lines not work ????? sigh\n",
    "result = []\n",
    "for i in range(dims_rescaled_data):\n",
    "    result.append(np.matmul(A_T,np.matmul(A,ev[:,i])/(np.linalg.norm(np.matmul(A,ev[:,i])))/np.sqrt(evals[i])))\n",
    "result = np.array(result).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssss = np.matmul(A,ev[:,0])/(np.linalg.norm(np.matmul(A,ev[:,0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss=np.matmul(A_T,np.matmul(A,ev[:,0])/(np.linalg.norm(np.matmul(A,ev[:,0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.std(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(evals[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(evals[:dims_rescaled_data])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_p = Av[:,0]/(np.linalg.norm(Av[:,0])*np.sqrt(abs(evals[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.mean(axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images: 60 and labels: 60\n",
      "(1, 91200)\n",
      "(2, 91200)\n",
      "(3, 91200)\n",
      "(4, 91200)\n",
      "(5, 91200)\n",
      "(6, 91200)\n",
      "(7, 91200)\n",
      "(8, 91200)\n",
      "(9, 91200)\n",
      "(10, 91200)\n",
      "(11, 91200)\n",
      "(12, 91200)\n",
      "(13, 91200)\n",
      "(14, 91200)\n",
      "(15, 91200)\n",
      "(16, 91200)\n",
      "(17, 91200)\n",
      "(18, 91200)\n",
      "(19, 91200)\n",
      "(20, 91200)\n",
      "(21, 91200)\n",
      "(22, 91200)\n",
      "(23, 91200)\n",
      "(24, 91200)\n",
      "(25, 91200)\n",
      "(26, 91200)\n",
      "(27, 91200)\n",
      "(28, 91200)\n",
      "(29, 91200)\n",
      "(30, 91200)\n",
      "(31, 91200)\n",
      "(32, 91200)\n",
      "(33, 91200)\n",
      "(34, 91200)\n",
      "(35, 91200)\n",
      "(36, 91200)\n",
      "(37, 91200)\n",
      "(38, 91200)\n",
      "(39, 91200)\n",
      "(40, 91200)\n",
      "(41, 91200)\n",
      "(42, 91200)\n",
      "(43, 91200)\n",
      "(44, 91200)\n",
      "(45, 91200)\n",
      "(46, 91200)\n",
      "(47, 91200)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "images, labels = load_data(data_dir=\"./CAFE/\")\n",
    "im = np.array(images[:48], 'float64')\n",
    "im_re = np.reshape(im, [len(im), -1])\n",
    "pca_result,evecs,evals,mean,vari = PCA(im_re, dims_rescaled_data=47)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting from array to PIL Image\n",
      "Converting from array to PIL Image\n",
      "Converting from array to PIL Image\n",
      "Converting from array to PIL Image\n",
      "Converting from array to PIL Image\n",
      "Converting from array to PIL Image\n"
     ]
    }
   ],
   "source": [
    "evecs_pic = np.reshape(evecs[0:6,:],[6,380,-1])\n",
    "def display_face(img):\n",
    "    \"\"\" Display the input image and optionally save as a PNG.\n",
    "\n",
    "    Args:\n",
    "    img: The NumPy array or image to display\n",
    "\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    # Convert img to PIL Image object (if it's an ndarray)\n",
    "    if type(img) == np.ndarray:\n",
    "        print(\"Converting from array to PIL Image\")\n",
    "        im = (img - img.min())*(255/(img.max()-img.min()))\n",
    "        # normalize the img into 0-255\n",
    "        img = Image.fromarray(im)\n",
    "    # Display the image\n",
    "    img.show()\n",
    "display_face(evecs_pic[0,:,:])\n",
    "display_face(evecs_pic[1,:,:])\n",
    "display_face(evecs_pic[2,:,:])\n",
    "display_face(evecs_pic[3,:,:])\n",
    "display_face(evecs_pic[4,:,:])\n",
    "display_face(evecs_pic[5,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def get_sad(images, labels):\n",
    "    image_sad = []\n",
    "    for i in range(len(images)):\n",
    "        if labels[i].find('_m') != -1:\n",
    "            image_sad.append(images[i])\n",
    "    return image_sad\n",
    "sad_vector = get_sad(images, labels)\n",
    "sad =np.array(sad_vector,'float64')\n",
    "def get_happy(images, labels):\n",
    "    image_happy = []\n",
    "    for i in range(len(images)):\n",
    "        if labels[i].find('_h') != -1:\n",
    "            image_happy.append(images[i])\n",
    "    return image_happy\n",
    "happy_vector = get_happy(images, labels)\n",
    "happy=np.array(happy_vector,'float64')\n",
    "from random import shuffle\n",
    "def shuffle_n_generate_data(train, target):\n",
    "    length = len(train)\n",
    "    train = np.array(train,'float64')\n",
    "    target = np.array(target)\n",
    "    ind_list = [i for i in range(length)]\n",
    "    shuffle(ind_list)\n",
    "    train_new  = train[ind_list,:,:]\n",
    "    target_new = target[ind_list]\n",
    "    return train_new, target_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.b.(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6931471805599453,\n",
       " 0.6800989500470918,\n",
       " 0.6686202746751178,\n",
       " 0.6584668824519779,\n",
       " 0.6494293079144096,\n",
       " 0.6413309266093525,\n",
       " 0.634024355845777,\n",
       " 0.6273873379545827,\n",
       " 0.6213187293584876,\n",
       " 0.6157348904083227]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handout_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x): \n",
    "    return 1. / (1. + np.exp(-x))\n",
    "def loss(t, y):\n",
    "    return -(t * np.log(y) +(1 - t) * np.log(1 - y)).mean()\n",
    "def accuracy(t,p):\n",
    "    return 1-1.0*sum(abs(t-(p>0.5)))/len(t)\n",
    "def logistic_regression(features,target,handout,handout_label, epoch, learning_rate):\n",
    "    ce_loss = []\n",
    "    best_weight = np.zeros(features.shape[1])  \n",
    "    weights = np.zeros(features.shape[1])\n",
    "    hand_out_mark = float('inf')\n",
    "    hand_out_loss = []\n",
    "    m = features.shape[0]\n",
    "    for step in range(epoch):\n",
    "        scores = np.dot(features, weights)\n",
    "        predictions = sigmoid(scores)\n",
    "        # record Cross-Entropy Error\n",
    "        ce_loss.append(loss(target,sigmoid(np.dot(features,weights))))\n",
    "        # Update weights with gradient \n",
    "        temp = loss(handout_label,sigmoid(np.dot(handout,weights)))\n",
    "        #print(sigmoid(np.dot(handout,weights)))\n",
    "        #print(handout_label)\n",
    "        #print(sigmoid(np.dot(handout,weights)))\n",
    "        if temp < hand_out_mark:\n",
    "            hand_out_mark = temp\n",
    "            best_weight = weights\n",
    "        gradient = 1/m*np.dot(features.T, predictions-target)\n",
    "        #print(accuracy(target,predictions))\n",
    "        weights -= learning_rate * gradient \n",
    "        hand_out_loss.append(temp)\n",
    "    return weights, best_weight,np.array(ce_loss),np.array(hand_out_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy is 1.0\n"
     ]
    }
   ],
   "source": [
    "image_logistic = np.concatenate((sad,happy),axis=0)\n",
    "label_logistic = np.concatenate(([0]*10,[1]*10),axis=0)\n",
    "[image_logistic_s,label_logistic_s] = shuffle_n_generate_data(image_logistic, label_logistic)\n",
    "\n",
    "devision = int(0.8*len(image_logistic_s))\n",
    "devision_2 = int(0.9*len(image_logistic_s))\n",
    "train_logistic  = image_logistic_s[:devision,:,:]\n",
    "train_label_logistic = label_logistic_s[:devision]\n",
    "handout_logistic = image_logistic_s[devision:devision_2,:,:]\n",
    "handout_label_logistic = label_logistic_s[devision:devision_2]\n",
    "test_logistic  = image_logistic_s[devision_2:,:,:]\n",
    "test_label_logistic = label_logistic_s [devision_2:]\n",
    "\n",
    "[features_pca,evect,evals,train_mean,train_std] = PCA(np.reshape(train_logistic,[len(train_logistic),-1]), dims_rescaled_data=15)\n",
    "handout_logistic_b = np.reshape(handout_logistic,[len(handout_logistic),-1])-train_mean\n",
    "handout_logistic_pca = np.matmul(handout_logistic_b,evect.T)\n",
    "\n",
    "[weight,best_weight,ce_loss,handout_loss] = logistic_regression(features_pca,train_label_logistic,handout_logistic_pca,handout_label_logistic,epoch=10, learning_rate=0.8)\n",
    "\n",
    "test_logistic_reshape = np.reshape(test_logistic,[len(test_logistic),-1])-train_mean\n",
    "test_logistic_pca = np.matmul(test_logistic_reshape,evect.T)\n",
    "test_prediction = sigmoid(np.dot(test_logistic_pca,weight))\n",
    "\n",
    "print('test accuracy is',accuracy(test_label_logistic,test_prediction))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_logistic = np.concatenate((sad,happy),axis=0)\n",
    "label_logistic = np.concatenate(([0]*10,[1]*10),axis=0)\n",
    "loss_train = []\n",
    "loss_val=[]\n",
    "test_accuracy = []\n",
    "for i in range (10):\n",
    "    [image_logistic_s,label_logistic_s] = shuffle_n_generate_data(image_logistic, label_logistic)\n",
    "    \n",
    "    devision = int(0.8*len(image_logistic_s))\n",
    "    devision_2 = int(0.9*len(image_logistic_s))\n",
    "    train_logistic  = image_logistic_s[:devision,:,:]\n",
    "    train_label_logistic = label_logistic_s[:devision]\n",
    "    handout_logistic = image_logistic_s[devision:devision_2,:,:]\n",
    "    handout_label_logistic = label_logistic_s[devision:devision_2]\n",
    "    test_logistic  = image_logistic_s[devision_2:,:,:]\n",
    "    test_label_logistic = label_logistic_s [devision_2:]\n",
    "    \n",
    "    [features_pca,evect,evals,train_mean,train_std] =\\\n",
    "        PCA(np.reshape(train_logistic,[len(train_logistic),-1]), dims_rescaled_data=15)\n",
    "    handout_logistic_b = np.reshape(handout_logistic,[len(handout_logistic),-1])-train_mean\n",
    "    handout_logistic_pca = np.matmul(handout_logistic_b,evect.T)\n",
    "\n",
    "    [weight,best_weight,ce_loss,handout_loss] = \\\n",
    "        logistic_regression(features_pca,train_label_logistic,\\\n",
    "        handout_logistic_pca,handout_label_logistic,epoch=10, learning_rate=0.8)\n",
    "\n",
    "    test_logistic_reshape = np.reshape(test_logistic,[len(test_logistic),-1])-train_mean\n",
    "    test_logistic_pca = np.matmul(test_logistic_reshape,evect.T)\n",
    "    test_prediction = sigmoid(np.dot(test_logistic_pca,weight))\n",
    "    \n",
    "    test_result = sigmoid(np.dot(test_logistic_pca,weight))\n",
    "    loss_train.append(ce_loss)\n",
    "    loss_val.append(handout_loss)\n",
    "    test_accuracy.append(accuracy(test_label_logistic,test_result))\n",
    "    #print(test_result)\n",
    "loss_avg = np.array(loss_train).mean(axis=0)\n",
    "loss_val_avg = np.array(loss_val).mean(axis=0)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.69314718, 0.68622681, 0.68085241, 0.67645932, 0.67270718,\n",
       "       0.66939176, 0.66638872, 0.66362048, 0.66103704, 0.65860503])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_val_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5, 0.5, 0.5, 0.5, 0.5, 1.0, 1.0, 0.5, 0.5, 1.0]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_softmax_re(images, labels):\n",
    "    train = []\n",
    "    target = []\n",
    "    for i in range(len(images)):\n",
    "        if labels[i].find('_ht') == -1 and labels[i].find('_n') == -1 :\n",
    "            train.append(images[i])\n",
    "            if labels[i].find('_h') !=-1:\n",
    "                target.append(0)\n",
    "            elif labels[i].find('_m') !=-1:\n",
    "                target.append(1)\n",
    "            elif labels[i].find('_s') !=-1:\n",
    "                target.append(2)\n",
    "            elif labels[i].find('_f') !=-1:\n",
    "                target.append(3)\n",
    "            elif labels[i].find('_a') !=-1:\n",
    "                target.append(4)\n",
    "            elif labels[i].find('_d') !=-1:\n",
    "                target.append(5)\n",
    "    return train, target\n",
    "def oneHot(Y,max_val):\n",
    "    result = []\n",
    "    for i in range(len(Y)):\n",
    "        onehot = [0]*(int(max_val)+1)\n",
    "        onehot[Y[i]] = 1\n",
    "        result.append(onehot)\n",
    "    return np.array(result)\n",
    "def cross_entropy(x, y):\n",
    "    \"\"\" Computes cross entropy between two distributions.\n",
    "    Input: x: iterabale of N non-negative values\n",
    "           y: iterabale of N non-negative values\n",
    "    Returns: scalar\n",
    "    \"\"\"\n",
    "\n",
    "    if np.any(x < 0) or np.any(y < 0):\n",
    "        raise ValueError('Negative values exist.')\n",
    "\n",
    "    # Force to proper probability mass function.\n",
    "    x = np.array(x, dtype=np.float)\n",
    "    y = np.array(y, dtype=np.float)\n",
    "    ##x /= np.sum(x)\n",
    "    ##y /= np.sum(y)\n",
    "\n",
    "    # Ignore zero 'y' elements.\n",
    "    mask = y > 0\n",
    "    #print(x)\n",
    "    #print(y)\n",
    "    x = x[mask]\n",
    "    y = y[mask]    \n",
    "    ce = -np.sum(np.multiply(x,np.log(y))) \n",
    "    return ce\n",
    "def softmax(x):\n",
    "    s = np.max(x, axis=1)\n",
    "    s = s[:, np.newaxis] # necessary step to do broadcasting\n",
    "    e_x = np.exp(x - s)\n",
    "    div = np.sum(e_x, axis=1)\n",
    "    div = div[:, np.newaxis] # dito\n",
    "    return e_x / div\n",
    "def accuracy_softmax(pred,target):\n",
    "    return sum(pred.argmax(axis=1)==target.argmax(axis=1))/pred.shape[0]\n",
    "       \n",
    "def softmax_regression(x,y,handout,handout_label,epoch,learningRate):\n",
    "    ce_loss = []\n",
    "    best_weight = np.zeros([x.shape[1],len(np.unique(y))])\n",
    "    hand_out_mark = float('inf')\n",
    "    hand_out_loss = []\n",
    "    weight = np.zeros([x.shape[1],len(np.unique(y))])\n",
    "    m = x.shape[0] #First we get the number of training examples\n",
    "    max_val = y.max() \n",
    "    y_mat = oneHot(y,max_val) #Next we convert the integer class coding into a one-hot representation\n",
    "    h_label = oneHot(handout_label,max_val)\n",
    "    for i in range(0,epoch):\n",
    "        scores = np.dot(x,weight) #Then we compute raw class scores given our input and current weights\n",
    "        prob = softmax(scores) #Next we perform a softmax on these scores to get their probabilities\n",
    "        #print(prob)\n",
    "        temp = cross_entropy(h_label,softmax(np.dot(handout,weight)))\n",
    "        #print(sigmoid(np.dot(handout,weights)))\n",
    "        #print(handout_label)\n",
    "        #print(accuracy_softmax(prob,y_mat))\n",
    "        if temp < hand_out_mark:\n",
    "            hand_out_mark = temp\n",
    "            best_weight = weight\n",
    "        loss = cross_entropy(np.array(y_mat),prob) #We then find the loss of the probabilities\n",
    "        grad = -np.dot(x.T,(y_mat-prob))  #And compute the gradient for that loss\n",
    "        ce_loss.append(loss)\n",
    "        hand_out_loss.append(temp)\n",
    "        weight = weight -(learningRate * grad)\n",
    "    return weight, best_weight,np.array(ce_loss),np.array(hand_out_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainning loss of 10is \n",
      "[86.00445452 77.78439615 71.0792518  65.44284162 60.62037836 56.44178476\n",
      " 52.78339589 49.5517255  46.6746257  44.09556635]\n",
      "validation loss of 10is \n",
      "[0.69314718 0.68400153 0.67389889 0.66321606 0.652246   0.64120534\n",
      " 0.63024794 0.61947896 0.60896716 0.5987548 ]\n",
      "trainning loss of 20is \n",
      "[86.00445452 77.77766586 71.14288814 65.58142158 60.8211709  56.69138558\n",
      " 53.07079041 49.8681064  47.01287112 44.44987961 42.13534068 40.03409577\n",
      " 38.11757908 36.36231191 34.74878481 33.26062139 31.88394736 30.60690956\n",
      " 29.4193053  28.31229355]\n",
      "validation loss of 20is \n",
      "[0.69314718 0.68400153 0.67389889 0.66321606 0.652246   0.64120534\n",
      " 0.63024794 0.61947896 0.60896716 0.5987548 ]\n",
      "trainning loss of 30is \n",
      "[86.00445452 77.94476523 71.48522938 66.0600781  61.39408157 57.32509784\n",
      " 53.74125294 50.5586133  47.71187737 45.14934999 42.82962585 40.71919078\n",
      " 38.79063396 37.02130401 35.39229489 33.88767719 32.49391188 31.19939975\n",
      " 29.9941319  28.86941596 27.81765934 26.83219586 25.90714551 25.03730009\n",
      " 24.21802879 23.44519991 22.71511513 22.02445418 21.37022789 20.74973803]\n",
      "validation loss of 30is \n",
      "[0.69314718 0.68400153 0.67389889 0.66321606 0.652246   0.64120534\n",
      " 0.63024794 0.61947896 0.60896716 0.5987548 ]\n",
      "trainning loss of 40is \n",
      "[86.00445452 77.85041192 71.28816166 65.78776833 61.07750755 56.98791805\n",
      " 53.39905723 50.22096496 47.38442404 44.83534759 42.53090153 40.43671248\n",
      " 38.52483591 36.77227096 35.15986858 33.67152171 32.29355783 31.01427731\n",
      " 29.82359771 28.71277577 27.67418731 26.70115077 25.78778413 24.92888782\n",
      " 24.11984802 23.35655617 22.63534157 21.9529145  21.30631816 20.69288753\n",
      " 20.1102144  19.55611711 19.02861461 18.52590395 18.04634068 17.58842183\n",
      " 17.15077102 16.73212538 16.33132411 15.94729828]\n",
      "validation loss of 40is \n",
      "[0.69314718 0.68400153 0.67389889 0.66321606 0.652246   0.64120534\n",
      " 0.63024794 0.61947896 0.60896716 0.5987548 ]\n",
      "trainning loss of 50is \n",
      "[86.00445452 78.03161883 71.61459218 66.23054017 61.61045419 57.58815046\n",
      " 54.04770578 50.90307079 48.08834681 45.5520794  43.2534696  41.15971568\n",
      " 39.24410134 37.48460074 35.86284933 34.3633764  32.97302661 31.68051926\n",
      " 30.47610843 29.351318   28.29873254 27.31183049 26.3848495  25.51267667\n",
      " 24.69075821 23.91502437 23.18182656 22.48788435 21.83024035 21.20622178\n",
      " 20.61340738 20.04959883 19.51279605 19.00117562 18.51307201 18.04696105\n",
      " 17.60144549 17.17524223 16.76717102 16.3761445  16.00115933 15.64128833\n",
      " 15.29567343 14.96351941 14.64408832 14.33669439 14.04069959 13.75550954\n",
      " 13.48056984 13.21536282]\n",
      "validation loss of 50is \n",
      "[0.69314718 0.68400153 0.67389889 0.66321606 0.652246   0.64120534\n",
      " 0.63024794 0.61947896 0.60896716 0.5987548 ]\n"
     ]
    }
   ],
   "source": [
    "for ep in [10,20,30,40,50]:    \n",
    "    loss_train = []\n",
    "    loss_val=[]\n",
    "    test_accuracy = []\n",
    "    for i in range(10):\n",
    "        [image_softmax, label_softmax] = get_data_for_softmax_re(images, labels)\n",
    "        image_softmax,label_softmax= shuffle_n_generate_data(image_softmax, label_softmax)\n",
    "        devision = int(0.8*len(image_softmax))\n",
    "        devision_2 = int(0.9*len(image_softmax))\n",
    "        train_softmax  = image_softmax[:devision,:,:]\n",
    "        train_label_softmax  = label_softmax[:devision]\n",
    "        handout_softmax  = image_softmax[devision:devision_2,:,:]\n",
    "        handout_label_softmax  = label_softmax[devision:devision_2]\n",
    "        test_softmax  = image_softmax[devision_2:,:,:]\n",
    "        test_label_softmax  = label_softmax[devision_2:]\n",
    "        \n",
    "        [features_pca,evect,evals,train_mean,train_std] =\\\n",
    "        PCA(np.reshape(train_softmax,[len(train_softmax),-1]), dims_rescaled_data=47)\n",
    "        \n",
    "        handout_softmax_b = np.reshape(handout_softmax,[len(handout_softmax),-1])-train_mean\n",
    "        handout_softmax_pca = np.matmul(handout_softmax_b,evect.T)\n",
    "        \n",
    "        [weight,best_weight, ce_loss,hand_out_loss] = softmax_regression(features_pca,train_label_softmax,handout_softmax_pca,handout_label_softmax,epoch=ep,learningRate=0.05)\n",
    "        \n",
    "        test_softmax_reshape = np.reshape(test_softmax,[len(test_softmax),-1])-train_mean\n",
    "        test_softmax_pca = np.matmul(test_softmax_reshape,evect.T)\n",
    "        test_prediction = softmax(np.dot(test_softmax_pca,weight))\n",
    "    \n",
    "        loss_train.append(ce_loss)\n",
    "        loss_val.append(handout_loss)\n",
    "        test_accuracy.append(accuracy_softmax(oneHot(test_label_softmax,train_label_softmax.max()),test_prediction))\n",
    "    \n",
    "    loss_avg = np.array(loss_train).mean(axis=0)\n",
    "    loss_val_avg = np.array(loss_val).mean(axis=0)  \n",
    "    print('trainning loss of ' + str(ep) + 'is ')\n",
    "    print(loss_avg)\n",
    "    print('validation loss of ' + str(ep) + 'is ')\n",
    "    print(loss_val_avg)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sofmat_regression_sdg(x,y, epoch,learningRate):\n",
    "    losses = []\n",
    "    weight = np.zeros([x.shape[1],len(np.unique(y))])\n",
    "    m = x.shape[0] #First we get the number of training examples\n",
    "    y_mat = oneHot(y) #Next we convert the integer class coding into a one-hot representation\n",
    "    ind_list = [i for i in range(m)]\n",
    "    for i in range(0,epoch):\n",
    "        shuffle(ind_list)\n",
    "        for j in range(m):\n",
    "            scores = np.dot(x[ind_list[j]],weight) #Then we compute raw class scores given our input and current weights\n",
    "            prob = softmax(scores) #Next we perform a softmax on these scores to get their probabilities\n",
    "            loss = -1*np.sum(y_mat[ind_list[j]]*np.log(prob)) #We then find the loss of the probabilities\n",
    "            #print(y_mat[ind_list[j]])\n",
    "            #print(prob.sum())\n",
    "            grad = -1*np.outer(x[ind_list[j]],(y_mat[ind_list[j]] - prob))#And compute the gradient for that loss\n",
    "            #print(prob)\n",
    "            losses.append(loss)\n",
    "            print(loss)\n",
    "            weight = weight + (learningRate * grad)\n",
    "    return weight, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 5, 5, 2, 2, 2])"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_label_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5,\n",
       " 0.3333333333333333,\n",
       " 0.5,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.5,\n",
       " 0.3333333333333333,\n",
       " 0.16666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.16666666666666666]"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 380, 240)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHot(Y):\n",
    "    result = []\n",
    "    for i in range(len(Y)):\n",
    "        onehot = [0]*(int(Y.max())+1)\n",
    "        onehot[Y[i]] = 1\n",
    "        result.append(onehot)\n",
    "    return result\n",
    "\n",
    "def cross_entropy(x, y):\n",
    "    \"\"\" Computes cross entropy between two distributions.\n",
    "    Input: x: iterabale of N non-negative values\n",
    "           y: iterabale of N non-negative values\n",
    "    Returns: scalar\n",
    "    \"\"\"\n",
    "\n",
    "    if np.any(x < 0) or np.any(y < 0):\n",
    "        raise ValueError('Negative values exist.')\n",
    "\n",
    "    # Force to proper probability mass function.\n",
    "    x = np.array(x, dtype=np.float)\n",
    "    y = np.array(y, dtype=np.float)\n",
    "    x /= np.sum(x)\n",
    "    y /= np.sum(y)\n",
    "\n",
    "    # Ignore zero 'y' elements.\n",
    "    mask = y > 0\n",
    "    x = x[mask]\n",
    "    y = y[mask]    \n",
    "    ce = -np.sum(x * np.log(y)) \n",
    "    return ce\n",
    "def softmax(x):\n",
    "    s = np.max(x, axis=1)\n",
    "    s = s[:, np.newaxis] # necessary step to do broadcasting\n",
    "    e_x = np.exp(x - s)\n",
    "    div = np.sum(e_x, axis=1)\n",
    "    div = div[:, np.newaxis] # dito\n",
    "    return e_x / div\n",
    "def sofmat_regression(x,y, epoch,learningRate):\n",
    "    losses = []\n",
    "    weight = np.zeros([x.shape[1],len(np.unique(y))])\n",
    "    m = x.shape[0] #First we get the number of training examples\n",
    "    y_mat = oneHot(y) #Next we convert the integer class coding into a one-hot representation\n",
    "    for i in range(0,epoch):\n",
    "        scores = np.dot(x,weight) #Then we compute raw class scores given our input and current weights\n",
    "        prob = softmax(scores) #Next we perform a softmax on these scores to get their probabilities\n",
    "        loss = -1/m/6*cross_entropy(np.array(y_mat),np.log(prob)) #We then find the loss of the probabilities\n",
    "        grad = -1/m/6*np.dot(x.T,(y_mat-prob))  #And compute the gradient for that loss\n",
    "        losses.append(loss)\n",
    "        print(loss)\n",
    "        weight = weight - (learningRate * grad)\n",
    "    return weight, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-163-196ee1fa9f66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeatures_softmax_pca\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevect_softmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_softmax\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_softmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdims_rescaled_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m47\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "features_softmax_pca, evect_softmax = PCA(np.reshape(train_softmax,[len(train_softmax),-1]), dims_rescaled_data=47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Negative values exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-604-f323ad8f2e6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_sofmax\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msofmat_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_softmax_pca\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_label_softmax\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearningRate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.000005\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-594-393322ff3642>\u001b[0m in \u001b[0;36msofmat_regression\u001b[0;34m(x, y, epoch, learningRate)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Then we compute raw class scores given our input and current weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Next we perform a softmax on these scores to get their probabilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_mat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#We then find the loss of the probabilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_mat\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#And compute the gradient for that loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-594-393322ff3642>\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Negative values exist.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Force to proper probability mass function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Negative values exist."
     ]
    }
   ],
   "source": [
    "[weight, loss_sofmax] = sofmat_regression(features_softmax_pca,train_label_softmax,epoch=50,learningRate=0.000005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x.T - np.max(x).T)\n",
    "    return np.transpose(e_x / e_x.sum())\n",
    "def sofmat_regression_sdg(x,y, epoch,learningRate):\n",
    "    losses = []\n",
    "    weight = np.zeros([x.shape[1],len(np.unique(y))])\n",
    "    m = x.shape[0] #First we get the number of training examples\n",
    "    y_mat = oneHot(y) #Next we convert the integer class coding into a one-hot representation\n",
    "    ind_list = [i for i in range(m)]\n",
    "    for i in range(0,epoch):\n",
    "        shuffle(ind_list)\n",
    "        for j in range(m):\n",
    "            scores = np.dot(x[ind_list[j]],weight) #Then we compute raw class scores given our input and current weights\n",
    "            prob = softmax(scores) #Next we perform a softmax on these scores to get their probabilities\n",
    "            loss = -1*np.sum(y_mat[ind_list[j]]*np.log(prob)) #We then find the loss of the probabilities\n",
    "            #print(y_mat[ind_list[j]])\n",
    "            #print(prob.sum())\n",
    "            grad = -1*np.outer(x[ind_list[j]],(y_mat[ind_list[j]] - prob))#And compute the gradient for that loss\n",
    "            #print(prob)\n",
    "            losses.append(loss)\n",
    "            print(loss)\n",
    "            weight = weight + (learningRate * grad)\n",
    "    return weight, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
