{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# CSE 253: Programming Assignment 1\n",
    "# Code snippet by Jenny Hamer\n",
    "# Winter 2019\n",
    "################################################################################\n",
    "# We've provided you with the dataset in CAFE.tar.gz. To uncompress, use:\n",
    "# tar -xzvf CAFE.tar.gz\n",
    "################################################################################\n",
    "# To install PIL, refer to the instructions for your system:\n",
    "# https://pillow.readthedocs.io/en/5.2.x/installation.html\n",
    "################################################################################\n",
    "# If you don't have NumPy installed, please use the instructions here:\n",
    "# https://scipy.org/install.html\n",
    "################################################################################\n",
    "\n",
    "from os import listdir\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from scipy import linalg as la\n",
    "\n",
    "# The relative path to your CAFE-Gamma dataset\n",
    "data_dir = \"./CAFE/\"\n",
    "\n",
    "# Dictionary of semantic \"label\" to emotions\n",
    "emotion_dict = {\"h\": \"happy\", \"ht\": \"happy with teeth\", \"m\": \"maudlin\",\n",
    "\t\"s\": \"surprise\", \"f\": \"fear\", \"a\": \"anger\", \"d\": \"disgust\", \"n\": \"neutral\"}\n",
    "\n",
    "\n",
    "def load_data(data_dir=\"./CAFE/\"):\n",
    "    \"\"\" Load all PGM images stored in your data directory into a list of NumPy\n",
    "    arrays with a list of corresponding labels.\n",
    "\n",
    "    Args:\n",
    "        data_dir: The relative filepath to the CAFE dataset.\n",
    "    Returns:\n",
    "        images: A list containing every image in CAFE as an array.\n",
    "        labels: A list of the corresponding labels (filenames) for each image.\n",
    "    \"\"\"\n",
    "    # Get the list of image file names\n",
    "    all_files = listdir(data_dir)\n",
    "\n",
    "    # Store the images as arrays and their labels in two lists\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    for file in all_files:\n",
    "    # Load in the files as PIL images and convert to NumPy arrays\n",
    "        if file.find('_ht') == -1 and file.find('_n') == -1:\n",
    "            img = Image.open(data_dir + file)\n",
    "            images.append(np.array(img))\n",
    "            labels.append(file)\n",
    "\n",
    "    print(\"Total number of images:\", len(images), \"and labels:\", len(labels))\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-047ed65ff157>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### TODO  DEVIDE Eigen VEctore to EigenCVALUE\n",
    "\n",
    "def PCA(data, dims_rescaled_data=2):\n",
    "    \"\"\"\n",
    "    returns: data transformed in 2 dims/columns + regenerated original data\n",
    "    pass in: data as 2D NumPy array\n",
    "    \"\"\"\n",
    "    # mean center the data\n",
    "    mean = data.mean(axis=1)\n",
    "    std = data.std(axis=1)\n",
    "    data = data-mean\n",
    "    data = data_T.transpose()\n",
    "\n",
    "    # calculate the covariance matrix\n",
    "    print(data_T.shape)\n",
    "    print(data.shape)\n",
    "    R = np.dot(data_T,data)\n",
    "    evals, evecs = LA.eigh(R)\n",
    "    # sort eigenvalue in decreasing order\n",
    "    idx = NP.argsort(evals)[::-1]\n",
    "    vect = NP.dot(data,evecs)\n",
    "    evecs = vect[:,idx]\n",
    "    # print(evecs.shape)\n",
    "    evals = evals[idx]\n",
    "    print(evals)\n",
    "    evecs = evecs/np.sqrt(evals) \n",
    "    evecs = evecs[:,:dims_rescaled_data]\n",
    "    ##vect = vect-vect.mean(axis = 0)\n",
    "    #vect = vect[:,:dims_rescaled_data]\n",
    "    result = NP.dot(data_T,evecs[:,0])\n",
    "    result = result/np.linalg.norm(evecs[:,0])\n",
    "    #result = result - result.mean(axis =0) \n",
    "    return result,evecs,evals,mean,std\n",
    "\n",
    "def test_PCA(data, dims_rescaled_data=2):\n",
    "    '''\n",
    "    test by attempting to recover original data array from\n",
    "    the eigenvectors of its covariance matrix & comparing that\n",
    "    'recovered' array with the original data\n",
    "    '''\n",
    "    _ , _ , eigenvectors = PCA(data, dim_rescaled_data=2)\n",
    "    data_recovered = NP.dot(eigenvectors, m).T\n",
    "    data_recovered += data_recovered.mean(axis=0)\n",
    "    assert NP.allclose(data, data_recovered)\n",
    "\n",
    "\n",
    "def plot_pca(data):\n",
    "    from matplotlib import pyplot as MPL\n",
    "    clr1 =  '#2026B2'\n",
    "    fig = MPL.figure()\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    data_resc, data_orig = PCA(data)\n",
    "    ax1.plot(data_resc[:, 0], data_resc[:, 1], '.', mfc=clr1, mec=clr1)\n",
    "    MPL.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "dims_rescaled_data=47\n",
    "A = im_re.T\n",
    "mean = A.mean(axis=1)\n",
    "std = A.std(axis=1)\n",
    "A = (A.T-mean).T\n",
    "A_T = A.T\n",
    "sm = np.matmul(A_T,A)\n",
    "evals, ev = la.eigh(sm)\n",
    "idx = np.argsort(evals)[::-1]\n",
    "evals = evals[idx]\n",
    "ev = ev[idx,:]\n",
    "Av = np.matmul(A,ev)\n",
    "evct = Av[:,:dims_rescaled_data]\n",
    "result = np.matmul(A_T,evct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1443375672974064"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.inner(A_T,v_p).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss=np.matmul(A.T,np.matmul(A,ev[0,:])/(np.linalg.norm(np.matmul(A,ev[0,:]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2112.0045336300404"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sss.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "582593048.5364782"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evals[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24136.964360426067"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(evals[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_p = Av[:,0]/(np.linalg.norm(Av[:,0])*np.sqrt(abs(evals[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91200,)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.mean(axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images: 60 and labels: 60\n",
      "(48, 91200)\n",
      "(91200, 48)\n",
      "[8.23494948e+10 7.87809813e+05 6.33172878e+05 3.26764563e+05\n",
      " 2.91863814e+05 2.56301671e+05 2.03300669e+05 1.91288501e+05\n",
      " 1.48104871e+05 1.14227680e+05 1.02537741e+05 8.65584207e+04\n",
      " 7.84449374e+04 6.75922432e+04 6.16872154e+04 5.91331019e+04\n",
      " 5.58296048e+04 4.80302070e+04 4.77297630e+04 4.47134600e+04\n",
      " 4.29948917e+04 3.86110331e+04 3.53881062e+04 3.31248011e+04\n",
      " 3.21962328e+04 2.83607552e+04 2.80496441e+04 2.64889040e+04\n",
      " 2.44715825e+04 2.35041584e+04 2.18761434e+04 2.11986624e+04\n",
      " 2.07822491e+04 1.96044163e+04 1.84735261e+04 1.80207862e+04\n",
      " 1.67368282e+04 1.63485663e+04 1.58332428e+04 1.48968599e+04\n",
      " 1.39348043e+04 1.28485074e+04 1.19914975e+04 1.11462246e+04\n",
      " 1.04630522e+04 1.02449745e+04 8.39249784e+03 7.54069013e+03]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "images, labels = load_data(data_dir=\"./CAFE/\")\n",
    "im = np.array(images[:48], 'float64')\n",
    "im_re = np.reshape(im, [len(im), -1])\n",
    "pca_result,evecs,evals,mean,vari = PCA(im_re.T, dims_rescaled_data=47)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91200, 47)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting from array to PIL Image\n",
      "Converting from array to PIL Image\n",
      "Converting from array to PIL Image\n",
      "Converting from array to PIL Image\n",
      "Converting from array to PIL Image\n",
      "Converting from array to PIL Image\n"
     ]
    }
   ],
   "source": [
    "evecs_pic = np.reshape(evecs.T[0:6,:],[6,380,-1])\n",
    "def display_face(img):\n",
    "    \"\"\" Display the input image and optionally save as a PNG.\n",
    "\n",
    "    Args:\n",
    "    img: The NumPy array or image to display\n",
    "\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    # Convert img to PIL Image object (if it's an ndarray)\n",
    "    if type(img) == np.ndarray:\n",
    "        print(\"Converting from array to PIL Image\")\n",
    "        im = (img - img.min())*(255/(img.max()-img.min()))\n",
    "        # normalize the img into 0-255\n",
    "        img = Image.fromarray(im)\n",
    "    # Display the image\n",
    "    img.show()\n",
    "display_face(evecs_pic[0,:,:])\n",
    "display_face(evecs_pic[1,:,:])\n",
    "display_face(evecs_pic[2,:,:])\n",
    "display_face(evecs_pic[3,:,:])\n",
    "display_face(evecs_pic[4,:,:])\n",
    "display_face(evecs_pic[5,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def get_sad(images, labels):\n",
    "    image_sad = []\n",
    "    for i in range(len(images)):\n",
    "        if labels[i].find('_m') != -1:\n",
    "            image_sad.append(images[i])\n",
    "    return image_sad\n",
    "sad_vector = get_sad(images, labels)\n",
    "sad =np.array(sad_vector,'float64')\n",
    "def get_happy(images, labels):\n",
    "    image_happy = []\n",
    "    for i in range(len(images)):\n",
    "        if labels[i].find('_h') != -1:\n",
    "            image_happy.append(images[i])\n",
    "    return image_happy\n",
    "happy_vector = get_happy(images, labels)\n",
    "happy=np.array(happy_vector,'float64')\n",
    "from random import shuffle\n",
    "def shuffle_n_generate_data(train, target):\n",
    "    length = len(train)\n",
    "    train = np.array(train,'float64')\n",
    "    target = np.array(target)\n",
    "    ind_list = [i for i in range(length)]\n",
    "    shuffle(ind_list)\n",
    "    train_new  = train[ind_list,:,:]\n",
    "    target_new = target[ind_list]\n",
    "    return train_new, target_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.b.(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22757.63780683967"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_pca.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x): \n",
    "    return 1. / (1. + np.exp(-x))\n",
    "def loss(t, y):\n",
    "    return -(t * np.log(y) +(1 - t) * np.log(1 - y)).mean()\n",
    "def accuracy(t,p):\n",
    "    return 1-1.0*sum(abs(t-(p>0.5)))/len(t)\n",
    "def logistic_regression(features,target,handout,handout_label, epoch, learning_rate):\n",
    "    ce_loss = []\n",
    "    best_weight = np.zeros(features.shape[1])  \n",
    "    weights = np.zeros(features.shape[1])\n",
    "    hand_out_mark = float('inf')\n",
    "    hand_out_loss = []\n",
    "    m = features.shape[0]\n",
    "    for step in range(epoch):\n",
    "        scores = np.dot(features, weights)\n",
    "        predictions = sigmoid(scores)\n",
    "        print(predictions)\n",
    "        # record Cross-Entropy Error\n",
    "        ce_loss.append(loss(target,sigmoid(np.dot(features,weights))))\n",
    "        # Update weights with gradient \n",
    "        ##temp = loss(handout_label,sigmoid(np.dot(handout,weights)))\n",
    "        #print(sigmoid(np.dot(handout,weights)))\n",
    "        ##if temp < hand_out_mark:\n",
    "        ##    hand_out_mark = temp\n",
    "        ##    best_weight = weights\n",
    "        ##print(sum(predictions>0.5))\n",
    "        gradient = 1/m*np.dot(features.T, predictions-target)\n",
    "        #print(gradient)\n",
    "        weights += learning_rate * gradient \n",
    "        ##hand_out_loss.append(temp)\n",
    "    return weights, best_weight,ce_loss,hand_out_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 91200)\n",
      "(91200, 16)\n",
      "[2.76398290e+10 2.84240879e+05 2.74237415e+05 1.47775000e+05\n",
      " 1.21057005e+05 1.07198668e+05 9.66391534e+04 7.44017702e+04\n",
      " 6.20724373e+04 4.61201388e+04 4.06192006e+04 3.36198852e+04\n",
      " 2.92923649e+04 2.42933753e+04 1.95410348e+04 1.59610960e+04]\n",
      "[0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  after removing the cwd from sys.path.\n",
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in multiply\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "image_logistic = np.concatenate((sad,happy),axis=0)\n",
    "label_logistic = np.concatenate(([0]*10,[1]*10),axis=0)\n",
    "[image_logistic_s,label_logistic_s] = shuffle_n_generate_data(image_logistic, label_logistic)\n",
    "\n",
    "devision = int(0.8*len(image_logistic_s))\n",
    "devision_2 = int(0.9*len(image_logistic_s))\n",
    "train_logistic  = image_logistic_s[:devision,:,:]\n",
    "train_label_logistic = label_logistic_s[:devision]\n",
    "handout_logistic = image_logistic_s[devision:devision_2,:,:]\n",
    "handout_label_logistic = label_logistic_s[devision:devision_2]\n",
    "test_logistic  = image_logistic_s[devision_2:,:,:]\n",
    "test_label_logistic = label_logistic_s [devision_2:]\n",
    "\n",
    "[features_pca,evect,evals,train_mean,train_std] = PCA(np.reshape(train_logistic,[len(train_logistic),-1]), dims_rescaled_data=15)\n",
    "handout_logistic_b = np.reshape(handout_logistic,[len(handout_logistic),-1])/train_std-train_mean\n",
    "handout_logistic_pca = np.matmul((handout_logistic_b),evect)\n",
    "[weight,best_weight,ce_loss,handout_loss] = logistic_regression(features_pca,train_label_logistic,handout_logistic_pca,handout_label_logistic,epoch=10, learning_rate=0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 15)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "handout_logistic_a = np.reshape(handout_logistic,[len(handout_logistic),-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[45053581.10101586, -3603112.80100734,  -735854.05450527,\n",
       "        -3979448.60718387, -3687271.41808088, -4494244.86819231,\n",
       "        -1241669.56956167, -3934353.6138574 , -3672394.78184577,\n",
       "        -3229022.74824177, -3547880.25504816, -3462365.49311423,\n",
       "        -3623258.47583446, -2680289.20982047, -3162415.20472226],\n",
       "       [45150856.73845325, -3617695.55988003,  -732429.24640976,\n",
       "        -3989378.7325359 , -3698747.55293883, -4501697.91999709,\n",
       "        -1240962.9921502 , -3942604.35813272, -3684597.70532424,\n",
       "        -3234966.09637943, -3554062.6670973 , -3469440.91868556,\n",
       "        -3630569.88038068, -2684939.51562853, -3168763.59291294]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handout_logistic_pca\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_logistic = np.concatenate((sad,happy),axis=0)\n",
    "label_logistic = np.concatenate(([0]*10,[1]*10),axis=0)\n",
    "loss_train = []\n",
    "loss_val=[]\n",
    "test_accuracy = []\n",
    "for i in range (10):\n",
    "    [image_logistic_s,label_logistic_s] = shuffle_n_generate_data(image_logistic, label_logistic)\n",
    "    \n",
    "    devision = int(0.8*len(image_logistic_s))\n",
    "    devision_2 = int(0.9*len(image_logistic_s))\n",
    "    train_logistic  = image_logistic_s[:devision,:,:]\n",
    "    train_label_logistic = label_logistic_s[:devision]\n",
    "    handout_logistic = image_logistic_s[devision:devision_2,:,:]\n",
    "    handout_label_logistic = label_logistic_s[devision:devision_2]\n",
    "    test_logistic  = image_logistic_s[devision_2:,:,:]\n",
    "    test_label_logistic = label_logistic_s [devision_2:]\n",
    "    \n",
    "    [features_pca,evect,train_mean,train_std] = \\\n",
    "    PCA(np.reshape(train_logistic,[len(train_logistic),-1]), dims_rescaled_data=15)\n",
    "    \n",
    "    handout_logistic = np.reshape(handout_logistic,[len(handout_logistic),-1])/train_std\n",
    "    handout_logistic_pca = np.dot(handout_logistic,evect.T)\n",
    "    \n",
    "    [weight,best_weight,ce_loss,handout_loss] = \\\n",
    "    logistic_regression(features_pca,labels_1,handout_logistic_pca,\\\n",
    "                        handout_label_logistic, epoch=10, learning_rate=0.1)\n",
    "    \n",
    "    loss_train.append(ce_loss)\n",
    "    loss_val.append(handout_loss)\n",
    "    \n",
    "    test_logistic = np.reshape(test_logistic,[len(test_logistic),-1])/train_std\n",
    "    test_logistic_pca = np.dot(test_logistic,evect.T)\n",
    "    \n",
    "    test_result = sigmoid(np.dot(test_logistic_pca,weight))\n",
    "    test_accuracy.append(accuracy(test_label_logistic,test_result))\n",
    "    #print(test_result)\n",
    "loss_avg = loss_overall.mean(axis=0)\n",
    "loss_val_avg = loss_val.mean(axis=0)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 840,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 1.0]"
      ]
     },
     "execution_count": 840,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO here for ten times, we need to random pick the training data and validation data.\n",
    "features = np.concatenate((sad[:8],happy[:8]),axis=0)\n",
    "labels_1 = np.concatenate(([0]*8,[1]*8),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_feature = np.concatenate((sad[7:8],happy[7:8]),axis=0)\n",
    "holdout_label = np.concatenate(([0],[1]),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_feature = np.concatenate((sad[8:9],happy[8:9]),axis=0)\n",
    "test_label = np.concatenate(([0],[1]),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16,)\n",
      "(16, 16)\n",
      "(16, 91200)\n",
      "(91200, 15)\n",
      "(16, 15)\n"
     ]
    }
   ],
   "source": [
    "features_pca, evect = PCA(features, dims_rescaled_data=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_overall=[]\n",
    "for i in range(10):\n",
    "    [weight,ce_loss] = logistic_regression(features_pca, labels_1, 10, 0.1, add_intercept = False)\n",
    "    loss_overall.append(ce_loss)\n",
    "    ## TODO need to know how many graph need to print."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_pca = np.dot(holdout_feature,evect)\n",
    "holdout_pca = holdout_pca - holdout_pca.mean()\n",
    "holdout_pca = holdout_pca / holdout_pca.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.59456151,  0.82955879,  1.50045876,  0.37629237,  0.27772601,\n",
       "        -0.08406059, -0.45896371,  0.04358075,  0.11146505, -0.29916364,\n",
       "        -0.98522272,  0.99948702, -0.06402932, -0.05704641,  0.29104213],\n",
       "       [-1.97691541,  2.04132495, -1.1899904 ,  0.40191635,  0.53067138,\n",
       "        -0.13887111,  0.37930887,  0.84190681,  0.20134868,  0.08671695,\n",
       "         0.31215654,  0.27197625, -0.68661055,  0.15992016, -0.12142244]])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "holdout_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_result = sigmoid(np.dot(holdout_pca,weight)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.1421452 , 0.67520279])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "holdout_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "def shuffle_n_generate_data(train, target):\n",
    "    length = len(train)\n",
    "    train = np.array(train,'float64')\n",
    "    target = np.array(target)\n",
    "    ind_list = [i for i in range(length)]\n",
    "    shuffle(ind_list)\n",
    "    train_new  = train[ind_list,:,:]\n",
    "    target_new = target[ind_list,]\n",
    "    return train_new, target_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_softmax_re(images, labels):\n",
    "    train = []\n",
    "    target = []\n",
    "    for i in range(len(images)):\n",
    "        if labels[i].find('_ht') == -1 and labels[i].find('_n') == -1 :\n",
    "            train.append(images[i])\n",
    "            if labels[i].find('_h') !=-1:\n",
    "                target.append(0)\n",
    "            elif labels[i].find('_m') !=-1:\n",
    "                target.append(1)\n",
    "            elif labels[i].find('_s') !=-1:\n",
    "                target.append(2)\n",
    "            elif labels[i].find('_f') !=-1:\n",
    "                target.append(3)\n",
    "            elif labels[i].find('_a') !=-1:\n",
    "                target.append(4)\n",
    "            elif labels[i].find('_d') !=-1:\n",
    "                target.append(5)\n",
    "    return train, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "[image_softmax, label_softmax] = get_data_for_softmax_re(images, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60,)\n"
     ]
    }
   ],
   "source": [
    "image_softmax,label_softmax= shuffle_n_generate_data(image_softmax, label_softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "devision = int(0.8*len(image_softmax))\n",
    "devision_2 = int(0.9*len(image_softmax))\n",
    "train_softmax  = image_softmax[:devision,:,:]\n",
    "train_label_softmax  = label_softmax[:devision]\n",
    "handout_softmax  = image_softmax[devision:devision_2,:,:]\n",
    "handout_label_softmax  = label_softmax[devision:devision_2]\n",
    "test_softmax  = image_softmax[devision_2:,:,:]\n",
    "test_label_softmax  = label_softmax[devision_2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 380, 240)"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_softmax.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHot(Y):\n",
    "    result = []\n",
    "    for i in range(len(Y)):\n",
    "        onehot = [0]*(int(Y.max())+1)\n",
    "        onehot[Y[i]] = 1\n",
    "        result.append(onehot)\n",
    "    return result\n",
    "\n",
    "def cross_entropy(x, y):\n",
    "    \"\"\" Computes cross entropy between two distributions.\n",
    "    Input: x: iterabale of N non-negative values\n",
    "           y: iterabale of N non-negative values\n",
    "    Returns: scalar\n",
    "    \"\"\"\n",
    "\n",
    "    if np.any(x < 0) or np.any(y < 0):\n",
    "        raise ValueError('Negative values exist.')\n",
    "\n",
    "    # Force to proper probability mass function.\n",
    "    x = np.array(x, dtype=np.float)\n",
    "    y = np.array(y, dtype=np.float)\n",
    "    x /= np.sum(x)\n",
    "    y /= np.sum(y)\n",
    "\n",
    "    # Ignore zero 'y' elements.\n",
    "    mask = y > 0\n",
    "    x = x[mask]\n",
    "    y = y[mask]    \n",
    "    ce = -np.sum(x * np.log(y)) \n",
    "    return ce\n",
    "def softmax(x):\n",
    "    s = np.max(x, axis=1)\n",
    "    s = s[:, np.newaxis] # necessary step to do broadcasting\n",
    "    e_x = np.exp(x - s)\n",
    "    div = np.sum(e_x, axis=1)\n",
    "    div = div[:, np.newaxis] # dito\n",
    "    return e_x / div\n",
    "def sofmat_regression(x,y, epoch,learningRate):\n",
    "    losses = []\n",
    "    weight = np.zeros([x.shape[1],len(np.unique(y))])\n",
    "    m = x.shape[0] #First we get the number of training examples\n",
    "    y_mat = oneHot(y) #Next we convert the integer class coding into a one-hot representation\n",
    "    for i in range(0,epoch):\n",
    "        scores = np.dot(x,weight) #Then we compute raw class scores given our input and current weights\n",
    "        prob = softmax(scores) #Next we perform a softmax on these scores to get their probabilities\n",
    "        loss = -1/m/6*cross_entropy(np.array(y_mat),np.log(prob)) #We then find the loss of the probabilities\n",
    "        grad = -1/m/6*np.dot(x.T,(y_mat-prob))  #And compute the gradient for that loss\n",
    "        losses.append(loss)\n",
    "        print(loss)\n",
    "        weight = weight - (learningRate * grad)\n",
    "    return weight, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48,)\n",
      "(48, 48)\n",
      "(48, 91200)\n",
      "(91200, 47)\n",
      "(48, 47)\n"
     ]
    }
   ],
   "source": [
    "features_softmax_pca, evect_softmax = PCA(np.reshape(train_softmax,[len(train_softmax),-1]), dims_rescaled_data=47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Negative values exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-604-f323ad8f2e6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_sofmax\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msofmat_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_softmax_pca\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_label_softmax\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearningRate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.000005\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-594-393322ff3642>\u001b[0m in \u001b[0;36msofmat_regression\u001b[0;34m(x, y, epoch, learningRate)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Then we compute raw class scores given our input and current weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Next we perform a softmax on these scores to get their probabilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_mat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#We then find the loss of the probabilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_mat\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#And compute the gradient for that loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-594-393322ff3642>\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Negative values exist.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Force to proper probability mass function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Negative values exist."
     ]
    }
   ],
   "source": [
    "[weight, loss_sofmax] = sofmat_regression(features_softmax_pca,train_label_softmax,epoch=50,learningRate=0.000005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x.T - np.max(x).T)\n",
    "    return np.transpose(e_x / e_x.sum())\n",
    "def sofmat_regression_sdg(x,y, epoch,learningRate):\n",
    "    losses = []\n",
    "    weight = np.zeros([x.shape[1],len(np.unique(y))])\n",
    "    m = x.shape[0] #First we get the number of training examples\n",
    "    y_mat = oneHot(y) #Next we convert the integer class coding into a one-hot representation\n",
    "    ind_list = [i for i in range(m)]\n",
    "    for i in range(0,epoch):\n",
    "        shuffle(ind_list)\n",
    "        for j in range(m):\n",
    "            scores = np.dot(x[ind_list[j]],weight) #Then we compute raw class scores given our input and current weights\n",
    "            prob = softmax(scores) #Next we perform a softmax on these scores to get their probabilities\n",
    "            loss = -1*np.sum(y_mat[ind_list[j]]*np.log(prob)) #We then find the loss of the probabilities\n",
    "            #print(y_mat[ind_list[j]])\n",
    "            #print(prob.sum())\n",
    "            grad = -1*np.outer(x[ind_list[j]],(y_mat[ind_list[j]] - prob))#And compute the gradient for that loss\n",
    "            #print(prob)\n",
    "            losses.append(loss)\n",
    "            print(loss)\n",
    "            weight = weight + (learningRate * grad)\n",
    "    return weight, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
