{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "tensor([  8.7200,  39.4473,   7.4256,   4.6427,  18.5127,  16.7321,  81.8677,\n",
      "         20.1627,  23.0240,  47.6843,  43.5628,  65.5006,  32.1226, 492.9207],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F \n",
    "\n",
    "\n",
    "image_info = pd.read_csv(\"/datasets/ChestXray-NIHCC/Data_Entry_2017.csv\")\n",
    "labels = image_info[\"Finding Labels\"]\n",
    "classes = {0: \"Atelectasis\", 1: \"Cardiomegaly\", 2: \"Effusion\", \n",
    "                3: \"Infiltration\", 4: \"Mass\", 5: \"Nodule\", 6: \"Pneumonia\", \n",
    "                7: \"Pneumothorax\", 8: \"Consolidation\", 9: \"Edema\", \n",
    "                10: \"Emphysema\", 11: \"Fibrosis\", \n",
    "                12: \"Pleural_Thickening\", 13: \"Hernia\",14:\"No Finding\" }\n",
    "class_label  = {v: k for k, v in classes.items()}\n",
    "label_stats = []\n",
    "for l in labels: \n",
    "    l_list = l.split('|')\n",
    "    \n",
    "    for z in l_list:\n",
    "        label_stats.append(class_label[z])\n",
    "\n",
    "data = label_stats\n",
    "\n",
    "# fixed bin size\n",
    "bins = np.arange(0, 16, 1) # fixed bin size\n",
    "\n",
    "plt.xlim([min(data)-1, max(data)+1])\n",
    "\n",
    "plt.hist(data, bins=bins, alpha=0.5)\n",
    "plt.title('label_stats')\n",
    "plt.xlabel('labels'\"\")\n",
    "plt.ylabel('count')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "counts, b, bars = plt.hist(data,bins=bins)\n",
    "print(len(counts))\n",
    "# print((counts))\n",
    "minus_counts = 112120 - counts\n",
    "# print(minus_counts)\n",
    "\n",
    "weight_bce = minus_counts/counts\n",
    "# print(weight_bce)\n",
    "ratio = 1.0*counts/sum(counts)\n",
    "\n",
    "weight_bce = weight_bce[0:-1]\n",
    "# print(weight_bce.shape)\n",
    "weight_temp = np.ones((len(weight_bce)))\n",
    "# print(weight_bce)\n",
    "# weight = np.c_[weight_temp, weight_bce]\n",
    "\n",
    "# print(weight)\n",
    "\n",
    "# weight_bce = (ratio[-1]/ratio)[0:-1]\n",
    "# print(weight_bce)\n",
    "# print(weight_bce)\n",
    "\n",
    "weight_bce = torch.from_numpy(np.array(weight_bce)).type(torch.cuda.FloatTensor)\n",
    "print(weight_bce)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "model = models.resnet34(pretrained=True)\n",
    "model.fc = nn.Sequential(nn.Linear(512, 14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = 0\n",
    "for child in model.children():\n",
    "    ct += 1\n",
    "    if ct < 9:\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is supported\n"
     ]
    }
   ],
   "source": [
    "from baseline_cnn import *\n",
    "\n",
    "\n",
    "# Setup: initialize the hyperparameters/variables\n",
    "# Setup: initialize the hyperparameters/variables\n",
    "num_epochs = 10           # Number of full passes through the dataset\n",
    "batch_size = 128          # Number of samples in each minibatch\n",
    "learning_rate = 0.001  \n",
    "seed = np.random.seed(1) # Seed the random number generator for reproducibility\n",
    "p_val = 0.1              # Percent of the overall dataset to reserve for validation\n",
    "p_test = 0.2             # Percent of the overall dataset to reserve for testing\n",
    "\n",
    "\n",
    "#TODO: Convert to Tensor - you can later add other transformations, such as Scaling here\n",
    "\n",
    "transform = transforms.Compose([\n",
    "        #transforms.RandomResizedCrop(224),\n",
    "        #transforms.RandomHorizontalFlip(),\n",
    "        transforms.Resize([224,224]),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "\n",
    "# Check if your system supports CUDA\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# Setup GPU optimization if CUDA is supported\n",
    "if use_cuda:\n",
    "    computing_device = torch.device(\"cuda\")\n",
    "    extras = {\"num_workers\": 1, \"pin_memory\": True}\n",
    "    print(\"CUDA is supported\")\n",
    "else: # Otherwise, train on the CPU\n",
    "    computing_device = torch.device(\"cpu\")\n",
    "    extras = False\n",
    "    print(\"CUDA NOT supported\")\n",
    "\n",
    "model = model\n",
    "model = model.to(computing_device)\n",
    "    \n",
    "# Setup the training, validation, and testing dataloaders\n",
    "train_loader, val_loader, test_loader = create_split_loaders(batch_size, seed, transform=transform, \n",
    "                                                             p_val=p_val, p_test=p_test,\n",
    "                                                             shuffle=True, show_sample=False, \n",
    "                                                             extras=extras)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight = weight_bce)\n",
    "\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.0001)\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model on CUDA? True\n"
     ]
    }
   ],
   "source": [
    "print(\"Model on CUDA?\", next(model.parameters()).is_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# label = np.asarray([[1, 0, 0], [0, 1, 1], [0, 1, 1]])\n",
    "# output = np.asarray([[0, 0.9, 0], [0, 0.9, 0], [0.9, 0, 0.6]])\n",
    "def prediction(labels, outputs):\n",
    "    threshold = 0.5\n",
    "    num_cor = 0\n",
    "    num_tp = 0\n",
    "    num_fp = 0\n",
    "    num_fn = 0\n",
    "    num_total = labels.shape[0]*labels.shape[1]\n",
    "    test_result = np.zeros((labels.shape))\n",
    "    \n",
    "    temp_position = np.where(outputs >= threshold)\n",
    "    \n",
    "    test_result[temp_position] = 1\n",
    "        \n",
    "    temp_position0 = np.array(np.where(test_result == 1.0)).T.tolist()\n",
    "    temp_position1 = np.array(np.where(labels == 1)).T.tolist()\n",
    "          \n",
    "    temp_position2 = np.array(np.where(test_result == 0.0)).T.tolist()\n",
    "    temp_position3 = np.array(np.where(labels == 0)).T.tolist()\n",
    "    \n",
    "    temp_position4 = np.array(np.where(labels == test_result)).T.tolist()\n",
    "    \n",
    "    \n",
    "    num_cor = len(temp_position4)\n",
    "    for element in temp_position0:\n",
    "        if element in temp_position1:\n",
    "            num_tp += 1\n",
    "    \n",
    "    for element in temp_position0:\n",
    "        if element in temp_position3:\n",
    "            num_fp += 1   \n",
    "    \n",
    "    for element in temp_position2:\n",
    "        if element in temp_position1:\n",
    "            num_fn += 1      \n",
    "            \n",
    "    print(num_cor)\n",
    "    \n",
    "    accuracy = num_cor/num_total\n",
    "    if num_fp + num_tp == 0:\n",
    "        precision = 0\n",
    "    else:\n",
    "        precision = num_tp/(num_fp + num_tp)\n",
    "    if num_tp + num_fn == 0:\n",
    "        recall = 0\n",
    "    else:\n",
    "        recall = num_tp/(num_tp + num_fn)\n",
    "    bcr = (precision + recall)/2\n",
    "    return accuracy, precision, recall, bcr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for i, (inputs, labels) in enumerate(train_loader, 0):\n",
    "                inputs = torch.squeeze(torch.stack([inputs,inputs,inputs], dim=1, out=None))\n",
    "                inputs = inputs.to(computing_device)\n",
    "                #zeros = torch.zeros([batch_size, 986])\n",
    "                #labels = torch.cat((labels,zeros), 1)\n",
    "                labels = labels.to(computing_device)\n",
    "                \n",
    "                #print(labels.shape)\n",
    "                \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    \n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                #running_corrects += torch.sum(preds == labels.data)\n",
    "                running_corrects = 0\n",
    "                output_np = torch.sigmoid(outputs).cpu().detach().numpy()\n",
    "                label_np = labels.cpu().detach().numpy()\n",
    "                accuracy, precision, recall, bcr = prediction(label_np, output_np)\n",
    "                print('accuracy, precision, recall', accuracy, precision, recall)\n",
    "                print(\"Loss is \",loss.item())\n",
    "                \n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/24\n",
      "----------\n",
      "764\n",
      "accuracy, precision, recall 0.4263392857142857 0.03721841332027424 0.4578313253012048\n",
      "Loss is  1.2390117645263672\n",
      "824\n",
      "accuracy, precision, recall 0.45982142857142855 0.05263157894736842 0.6190476190476191\n",
      "Loss is  1.3361408710479736\n",
      "816\n",
      "accuracy, precision, recall 0.45535714285714285 0.05740181268882175 0.5876288659793815\n",
      "Loss is  1.378330111503601\n",
      "820\n",
      "accuracy, precision, recall 0.4575892857142857 0.046583850931677016 0.46875\n",
      "Loss is  1.3992644548416138\n",
      "845\n",
      "accuracy, precision, recall 0.47154017857142855 0.06190975865687303 0.5267857142857143\n",
      "Loss is  1.4761631488800049\n",
      "836\n",
      "accuracy, precision, recall 0.46651785714285715 0.05345911949685535 0.49038461538461536\n",
      "Loss is  1.3637497425079346\n",
      "858\n",
      "accuracy, precision, recall 0.47879464285714285 0.0574468085106383 0.5294117647058824\n",
      "Loss is  1.4478956460952759\n",
      "845\n",
      "accuracy, precision, recall 0.47154017857142855 0.05152471083070452 0.5212765957446809\n",
      "Loss is  1.2572377920150757\n",
      "847\n",
      "accuracy, precision, recall 0.47265625 0.03699788583509514 0.5072463768115942\n",
      "Loss is  1.2240132093429565\n",
      "870\n",
      "accuracy, precision, recall 0.48549107142857145 0.053475935828877004 0.5747126436781609\n",
      "Loss is  1.2483327388763428\n",
      "859\n",
      "accuracy, precision, recall 0.47935267857142855 0.045454545454545456 0.45161290322580644\n",
      "Loss is  1.2576526403427124\n",
      "849\n",
      "accuracy, precision, recall 0.47377232142857145 0.05732484076433121 0.4954128440366973\n",
      "Loss is  1.3174023628234863\n",
      "881\n",
      "accuracy, precision, recall 0.4916294642857143 0.044101433296582136 0.47619047619047616\n",
      "Loss is  1.28323233127594\n",
      "884\n",
      "accuracy, precision, recall 0.49330357142857145 0.060109289617486336 0.5339805825242718\n",
      "Loss is  1.3643144369125366\n",
      "913\n",
      "accuracy, precision, recall 0.5094866071428571 0.055865921787709494 0.5952380952380952\n",
      "Loss is  1.4794496297836304\n",
      "923\n",
      "accuracy, precision, recall 0.5150669642857143 0.05788876276958002 0.5666666666666667\n",
      "Loss is  1.28586745262146\n",
      "928\n",
      "accuracy, precision, recall 0.5178571428571429 0.05226480836236934 0.4838709677419355\n",
      "Loss is  1.2078444957733154\n",
      "917\n",
      "accuracy, precision, recall 0.51171875 0.05221339387060159 0.5348837209302325\n",
      "Loss is  1.3734437227249146\n",
      "909\n",
      "accuracy, precision, recall 0.5072544642857143 0.05417607223476298 0.5161290322580645\n",
      "Loss is  1.5504955053329468\n",
      "912\n",
      "accuracy, precision, recall 0.5089285714285714 0.05707762557077625 0.4807692307692308\n",
      "Loss is  1.2983970642089844\n",
      "918\n",
      "accuracy, precision, recall 0.5122767857142857 0.05636978579481398 0.5747126436781609\n",
      "Loss is  1.1862605810165405\n",
      "911\n",
      "accuracy, precision, recall 0.5083705357142857 0.052571428571428575 0.46938775510204084\n",
      "Loss is  1.4589016437530518\n",
      "949\n",
      "accuracy, precision, recall 0.5295758928571429 0.056338028169014086 0.5517241379310345\n",
      "Loss is  1.6161246299743652\n",
      "969\n",
      "accuracy, precision, recall 0.5407366071428571 0.0768321513002364 0.6074766355140186\n",
      "Loss is  1.356745958328247\n",
      "969\n",
      "accuracy, precision, recall 0.5407366071428571 0.05953827460510328 0.5\n",
      "Loss is  1.2577400207519531\n",
      "919\n",
      "accuracy, precision, recall 0.5128348214285714 0.06612529002320186 0.456\n",
      "Loss is  1.5425094366073608\n",
      "966\n",
      "accuracy, precision, recall 0.5390625 0.06115107913669065 0.5425531914893617\n",
      "Loss is  1.6932251453399658\n",
      "921\n",
      "accuracy, precision, recall 0.5139508928571429 0.05619266055045872 0.5051546391752577\n",
      "Loss is  1.3444076776504517\n",
      "936\n",
      "accuracy, precision, recall 0.5223214285714286 0.06198830409356725 0.4953271028037383\n",
      "Loss is  1.3362915515899658\n",
      "942\n",
      "accuracy, precision, recall 0.5256696428571429 0.06896551724137931 0.6\n",
      "Loss is  1.341118335723877\n",
      "909\n",
      "accuracy, precision, recall 0.5072544642857143 0.03995433789954338 0.45454545454545453\n",
      "Loss is  1.1585605144500732\n",
      "901\n",
      "accuracy, precision, recall 0.5027901785714286 0.05442176870748299 0.45714285714285713\n",
      "Loss is  1.3933759927749634\n",
      "897\n",
      "accuracy, precision, recall 0.5005580357142857 0.048726467331118496 0.55\n",
      "Loss is  1.190665602684021\n",
      "891\n",
      "accuracy, precision, recall 0.49720982142857145 0.04835164835164835 0.5569620253164557\n",
      "Loss is  1.1314882040023804\n",
      "920\n",
      "accuracy, precision, recall 0.5133928571428571 0.045714285714285714 0.5194805194805194\n",
      "Loss is  1.1065444946289062\n",
      "919\n",
      "accuracy, precision, recall 0.5128348214285714 0.0625 0.46153846153846156\n",
      "Loss is  1.5266146659851074\n",
      "918\n",
      "accuracy, precision, recall 0.5122767857142857 0.062146892655367235 0.5555555555555556\n",
      "Loss is  1.1793262958526611\n",
      "929\n",
      "accuracy, precision, recall 0.5184151785714286 0.059496567505720827 0.5591397849462365\n",
      "Loss is  1.2898873090744019\n",
      "925\n",
      "accuracy, precision, recall 0.5161830357142857 0.05454545454545454 0.5783132530120482\n",
      "Loss is  1.0758962631225586\n",
      "916\n",
      "accuracy, precision, recall 0.5111607142857143 0.04707233065442021 0.47126436781609193\n",
      "Loss is  1.2254852056503296\n",
      "951\n",
      "accuracy, precision, recall 0.5306919642857143 0.06524317912218268 0.5092592592592593\n",
      "Loss is  1.3760664463043213\n",
      "928\n",
      "accuracy, precision, recall 0.5178571428571429 0.056 0.5632183908045977\n",
      "Loss is  1.2092922925949097\n",
      "933\n",
      "accuracy, precision, recall 0.5206473214285714 0.061714285714285715 0.5869565217391305\n",
      "Loss is  1.2644991874694824\n",
      "948\n",
      "accuracy, precision, recall 0.5290178571428571 0.05200945626477541 0.5116279069767442\n",
      "Loss is  1.3712962865829468\n",
      "928\n",
      "accuracy, precision, recall 0.5178571428571429 0.04413472706155633 0.4810126582278481\n",
      "Loss is  1.1639169454574585\n",
      "904\n",
      "accuracy, precision, recall 0.5044642857142857 0.05239179954441914 0.45098039215686275\n",
      "Loss is  1.4835811853408813\n",
      "962\n",
      "accuracy, precision, recall 0.5368303571428571 0.06115107913669065 0.5204081632653061\n",
      "Loss is  1.5508805513381958\n",
      "937\n",
      "accuracy, precision, recall 0.5228794642857143 0.06336405529953917 0.5670103092783505\n",
      "Loss is  1.365664005279541\n",
      "964\n",
      "accuracy, precision, recall 0.5379464285714286 0.073512252042007 0.6494845360824743\n",
      "Loss is  1.326822280883789\n",
      "964\n",
      "accuracy, precision, recall 0.5379464285714286 0.05143540669856459 0.5512820512820513\n",
      "Loss is  1.1714718341827393\n",
      "960\n",
      "accuracy, precision, recall 0.5357142857142857 0.056490384615384616 0.5\n",
      "Loss is  1.3097338676452637\n",
      "949\n",
      "accuracy, precision, recall 0.5295758928571429 0.04801920768307323 0.4444444444444444\n",
      "Loss is  1.3918814659118652\n",
      "971\n",
      "accuracy, precision, recall 0.5418526785714286 0.06634499396863691 0.5392156862745098\n",
      "Loss is  1.236911654472351\n",
      "940\n",
      "accuracy, precision, recall 0.5245535714285714 0.05957943925233645 0.5204081632653061\n",
      "Loss is  1.268585205078125\n",
      "946\n",
      "accuracy, precision, recall 0.5279017857142857 0.05411764705882353 0.5227272727272727\n",
      "Loss is  1.1540485620498657\n",
      "953\n",
      "accuracy, precision, recall 0.5318080357142857 0.04561824729891957 0.4634146341463415\n",
      "Loss is  1.477820873260498\n",
      "969\n",
      "accuracy, precision, recall 0.5407366071428571 0.057971014492753624 0.5274725274725275\n",
      "Loss is  1.679317593574524\n",
      "981\n",
      "accuracy, precision, recall 0.5474330357142857 0.06487148102815178 0.53\n",
      "Loss is  1.2332066297531128\n",
      "977\n",
      "accuracy, precision, recall 0.5452008928571429 0.05481120584652863 0.5357142857142857\n",
      "Loss is  1.2682127952575684\n",
      "976\n",
      "accuracy, precision, recall 0.5446428571428571 0.06144578313253012 0.5795454545454546\n",
      "Loss is  1.202427625656128\n",
      "982\n",
      "accuracy, precision, recall 0.5479910714285714 0.06349206349206349 0.5473684210526316\n",
      "Loss is  1.4353655576705933\n",
      "985\n",
      "accuracy, precision, recall 0.5496651785714286 0.04950495049504951 0.5063291139240507\n",
      "Loss is  1.1630454063415527\n",
      "1008\n",
      "accuracy, precision, recall 0.5625 0.0599250936329588 0.6075949367088608\n",
      "Loss is  1.0482609272003174\n",
      "1010\n",
      "accuracy, precision, recall 0.5636160714285714 0.05660377358490566 0.5844155844155844\n",
      "Loss is  1.1517809629440308\n",
      "1020\n",
      "accuracy, precision, recall 0.5691964285714286 0.05215123859191656 0.47058823529411764\n",
      "Loss is  1.1150343418121338\n",
      "1045\n",
      "accuracy, precision, recall 0.5831473214285714 0.05540540540540541 0.4606741573033708\n",
      "Loss is  1.2992290258407593\n",
      "1034\n",
      "accuracy, precision, recall 0.5770089285714286 0.048 0.45\n",
      "Loss is  1.2422826290130615\n",
      "1040\n",
      "accuracy, precision, recall 0.5803571428571429 0.06266666666666666 0.4895833333333333\n",
      "Loss is  1.2107586860656738\n",
      "1070\n",
      "accuracy, precision, recall 0.5970982142857143 0.06694560669456066 0.4752475247524752\n",
      "Loss is  1.25973641872406\n",
      "1080\n",
      "accuracy, precision, recall 0.6026785714285714 0.05187319884726225 0.4\n",
      "Loss is  1.301310420036316\n",
      "1067\n",
      "accuracy, precision, recall 0.5954241071428571 0.06056338028169014 0.42574257425742573\n",
      "Loss is  1.415779709815979\n",
      "1065\n",
      "accuracy, precision, recall 0.5943080357142857 0.07063711911357341 0.4766355140186916\n",
      "Loss is  1.2704565525054932\n",
      "1077\n",
      "accuracy, precision, recall 0.6010044642857143 0.06896551724137931 0.5555555555555556\n",
      "Loss is  1.2362807989120483\n",
      "1074\n",
      "accuracy, precision, recall 0.5993303571428571 0.07012622720897616 0.47619047619047616\n",
      "Loss is  1.3477741479873657\n",
      "1068\n",
      "accuracy, precision, recall 0.5959821428571429 0.06102635228848821 0.4835164835164835\n",
      "Loss is  1.4437806606292725\n",
      "1063\n",
      "accuracy, precision, recall 0.5931919642857143 0.059065934065934064 0.4942528735632184\n",
      "Loss is  1.2740776538848877\n",
      "1072\n",
      "accuracy, precision, recall 0.5982142857142857 0.06064880112834979 0.44329896907216493\n",
      "Loss is  1.098936915397644\n",
      "1058\n",
      "accuracy, precision, recall 0.5904017857142857 0.07352941176470588 0.5729166666666666\n",
      "Loss is  1.2900937795639038\n",
      "1044\n",
      "accuracy, precision, recall 0.5825892857142857 0.06032171581769437 0.4891304347826087\n",
      "Loss is  1.8153455257415771\n",
      "1062\n",
      "accuracy, precision, recall 0.5926339285714286 0.06310013717421124 0.4946236559139785\n",
      "Loss is  1.2819468975067139\n",
      "1075\n",
      "accuracy, precision, recall 0.5998883928571429 0.05702364394993046 0.5125\n",
      "Loss is  1.1888986825942993\n",
      "1087\n",
      "accuracy, precision, recall 0.6065848214285714 0.07468879668049792 0.6\n",
      "Loss is  1.623140573501587\n",
      "1039\n",
      "accuracy, precision, recall 0.5797991071428571 0.05787348586810229 0.4479166666666667\n",
      "Loss is  1.6877989768981934\n",
      "1047\n",
      "accuracy, precision, recall 0.5842633928571429 0.08970976253298153 0.5528455284552846\n",
      "Loss is  1.7336686849594116\n",
      "1059\n",
      "accuracy, precision, recall 0.5909598214285714 0.056164383561643834 0.4823529411764706\n",
      "Loss is  1.2851512432098389\n",
      "1022\n",
      "accuracy, precision, recall 0.5703125 0.05789473684210526 0.4489795918367347\n",
      "Loss is  1.3694227933883667\n",
      "1042\n",
      "accuracy, precision, recall 0.5814732142857143 0.0610079575596817 0.5227272727272727\n",
      "Loss is  1.2577013969421387\n",
      "1028\n",
      "accuracy, precision, recall 0.5736607142857143 0.06060606060606061 0.4742268041237113\n",
      "Loss is  1.3721543550491333\n",
      "1051\n",
      "accuracy, precision, recall 0.5864955357142857 0.06200527704485488 0.6103896103896104\n",
      "Loss is  1.2562329769134521\n",
      "1033\n",
      "accuracy, precision, recall 0.5764508928571429 0.04979811574697174 0.4111111111111111\n",
      "Loss is  1.246924877166748\n",
      "990\n",
      "accuracy, precision, recall 0.5524553571428571 0.04720496894409938 0.5205479452054794\n",
      "Loss is  1.125870704650879\n",
      "1026\n",
      "accuracy, precision, recall 0.5725446428571429 0.0693196405648267 0.5684210526315789\n",
      "Loss is  1.2761740684509277\n",
      "1073\n",
      "accuracy, precision, recall 0.5987723214285714 0.06784260515603799 0.6097560975609756\n",
      "Loss is  1.3555225133895874\n",
      "1019\n",
      "accuracy, precision, recall 0.5686383928571429 0.05283505154639175 0.5189873417721519\n",
      "Loss is  1.25116765499115\n",
      "1014\n",
      "accuracy, precision, recall 0.5658482142857143 0.05928853754940711 0.41284403669724773\n",
      "Loss is  1.4571746587753296\n",
      "1001\n",
      "accuracy, precision, recall 0.55859375 0.04481434058898848 0.4375\n",
      "Loss is  1.2256475687026978\n",
      "1033\n",
      "accuracy, precision, recall 0.5764508928571429 0.059602649006622516 0.4787234042553192\n",
      "Loss is  1.344223141670227\n",
      "1017\n",
      "accuracy, precision, recall 0.5675223214285714 0.06080206985769728 0.4895833333333333\n",
      "Loss is  1.2731157541275024\n",
      "1029\n",
      "accuracy, precision, recall 0.57421875 0.06762028608582575 0.5306122448979592\n",
      "Loss is  1.4097517728805542\n",
      "1013\n",
      "accuracy, precision, recall 0.5652901785714286 0.054474708171206226 0.45652173913043476\n",
      "Loss is  1.2490136623382568\n",
      "1043\n",
      "accuracy, precision, recall 0.58203125 0.04946524064171123 0.49333333333333335\n",
      "Loss is  1.2012543678283691\n",
      "999\n",
      "accuracy, precision, recall 0.5574776785714286 0.06153846153846154 0.44036697247706424\n",
      "Loss is  1.7418673038482666\n",
      "1020\n",
      "accuracy, precision, recall 0.5691964285714286 0.06135770234986945 0.47\n",
      "Loss is  1.3202444314956665\n",
      "1022\n",
      "accuracy, precision, recall 0.5703125 0.05813953488372093 0.5232558139534884\n",
      "Loss is  1.1630486249923706\n",
      "1013\n",
      "accuracy, precision, recall 0.5652901785714286 0.05449936628643853 0.5657894736842105\n",
      "Loss is  1.198014736175537\n",
      "1004\n",
      "accuracy, precision, recall 0.5602678571428571 0.04409857328145266 0.4\n",
      "Loss is  1.3144892454147339\n",
      "1045\n",
      "accuracy, precision, recall 0.5831473214285714 0.0712401055408971 0.5567010309278351\n",
      "Loss is  1.225836157798767\n",
      "1037\n",
      "accuracy, precision, recall 0.5786830357142857 0.06806282722513089 0.5473684210526316\n",
      "Loss is  1.2297438383102417\n",
      "1020\n",
      "accuracy, precision, recall 0.5691964285714286 0.059508408796895215 0.5054945054945055\n",
      "Loss is  1.4307390451431274\n",
      "1024\n",
      "accuracy, precision, recall 0.5714285714285714 0.056578947368421055 0.4574468085106383\n",
      "Loss is  1.8132630586624146\n",
      "1019\n",
      "accuracy, precision, recall 0.5686383928571429 0.06727037516170763 0.5\n",
      "Loss is  1.3266806602478027\n",
      "1011\n",
      "accuracy, precision, recall 0.5641741071428571 0.057034220532319393 0.5487804878048781\n",
      "Loss is  1.457374930381775\n",
      "1006\n",
      "accuracy, precision, recall 0.5613839285714286 0.05660377358490566 0.5555555555555556\n",
      "Loss is  1.1313021183013916\n",
      "1003\n",
      "accuracy, precision, recall 0.5597098214285714 0.05 0.5797101449275363\n",
      "Loss is  1.121569037437439\n",
      "1016\n",
      "accuracy, precision, recall 0.5669642857142857 0.07216494845360824 0.5\n",
      "Loss is  1.4526093006134033\n",
      "1026\n",
      "accuracy, precision, recall 0.5725446428571429 0.07270408163265306 0.59375\n",
      "Loss is  1.2129430770874023\n",
      "1003\n",
      "accuracy, precision, recall 0.5597098214285714 0.05357142857142857 0.47191011235955055\n",
      "Loss is  1.160990595817566\n",
      "1020\n",
      "accuracy, precision, recall 0.5691964285714286 0.05913272010512484 0.44554455445544555\n",
      "Loss is  1.3216911554336548\n",
      "987\n",
      "accuracy, precision, recall 0.55078125 0.05583126550868486 0.5056179775280899\n",
      "Loss is  1.268979549407959\n",
      "1001\n",
      "accuracy, precision, recall 0.55859375 0.0688360450563204 0.5392156862745098\n",
      "Loss is  1.327669620513916\n",
      "999\n",
      "accuracy, precision, recall 0.5574776785714286 0.05764411027568922 0.5287356321839081\n",
      "Loss is  1.2746323347091675\n",
      "997\n",
      "accuracy, precision, recall 0.5563616071428571 0.07133917396745933 0.5181818181818182\n",
      "Loss is  1.3588911294937134\n",
      "992\n",
      "accuracy, precision, recall 0.5535714285714286 0.06593406593406594 0.6067415730337079\n",
      "Loss is  1.2076752185821533\n",
      "978\n",
      "accuracy, precision, recall 0.5457589285714286 0.05365853658536585 0.5365853658536586\n",
      "Loss is  1.266090750694275\n",
      "1005\n",
      "accuracy, precision, recall 0.5608258928571429 0.07257072570725707 0.6413043478260869\n",
      "Loss is  1.1861968040466309\n",
      "982\n",
      "accuracy, precision, recall 0.5479910714285714 0.05961070559610705 0.5697674418604651\n",
      "Loss is  1.2123514413833618\n",
      "981\n",
      "accuracy, precision, recall 0.5474330357142857 0.060196560196560195 0.5157894736842106\n",
      "Loss is  1.5866039991378784\n",
      "1013\n",
      "accuracy, precision, recall 0.5652901785714286 0.05897435897435897 0.5054945054945055\n",
      "Loss is  1.388678789138794\n",
      "1002\n",
      "accuracy, precision, recall 0.5591517857142857 0.07054455445544554 0.59375\n",
      "Loss is  1.1972140073776245\n",
      "958\n",
      "accuracy, precision, recall 0.5345982142857143 0.04994054696789536 0.5454545454545454\n",
      "Loss is  1.2814582586288452\n",
      "990\n",
      "accuracy, precision, recall 0.5524553571428571 0.06343283582089553 0.51\n",
      "Loss is  1.2660648822784424\n",
      "988\n",
      "accuracy, precision, recall 0.5513392857142857 0.06451612903225806 0.5098039215686274\n",
      "Loss is  1.6892093420028687\n",
      "952\n",
      "accuracy, precision, recall 0.53125 0.06323185011709602 0.574468085106383\n",
      "Loss is  1.3575050830841064\n",
      "948\n",
      "accuracy, precision, recall 0.5290178571428571 0.05875440658049354 0.5376344086021505\n",
      "Loss is  1.2204792499542236\n",
      "977\n",
      "accuracy, precision, recall 0.5452008928571429 0.06287787182587666 0.5652173913043478\n",
      "Loss is  1.2220909595489502\n",
      "994\n",
      "accuracy, precision, recall 0.5546875 0.056790123456790124 0.575\n",
      "Loss is  1.1324769258499146\n",
      "994\n",
      "accuracy, precision, recall 0.5546875 0.0695970695970696 0.6129032258064516\n",
      "Loss is  1.2644834518432617\n",
      "999\n",
      "accuracy, precision, recall 0.5574776785714286 0.06781750924784218 0.5978260869565217\n",
      "Loss is  1.1283506155014038\n",
      "969\n",
      "accuracy, precision, recall 0.5407366071428571 0.060714285714285714 0.6\n",
      "Loss is  1.340544581413269\n",
      "957\n",
      "accuracy, precision, recall 0.5340401785714286 0.06205250596658711 0.5148514851485149\n",
      "Loss is  1.3290283679962158\n",
      "1002\n",
      "accuracy, precision, recall 0.5591517857142857 0.05520702634880803 0.5432098765432098\n",
      "Loss is  1.200449824333191\n",
      "994\n",
      "accuracy, precision, recall 0.5546875 0.07777777777777778 0.5526315789473685\n",
      "Loss is  1.313802719116211\n",
      "1016\n",
      "accuracy, precision, recall 0.5669642857142857 0.08060453400503778 0.5818181818181818\n",
      "Loss is  1.3802381753921509\n",
      "1015\n",
      "accuracy, precision, recall 0.56640625 0.054637865311308764 0.5657894736842105\n",
      "Loss is  1.1499309539794922\n",
      "1004\n",
      "accuracy, precision, recall 0.5602678571428571 0.06625 0.5638297872340425\n",
      "Loss is  1.1912131309509277\n",
      "1012\n",
      "accuracy, precision, recall 0.5647321428571429 0.07231920199501247 0.6170212765957447\n",
      "Loss is  1.2447420358657837\n",
      "1008\n",
      "accuracy, precision, recall 0.5625 0.06781750924784218 0.6626506024096386\n",
      "Loss is  1.152612566947937\n",
      "1024\n",
      "accuracy, precision, recall 0.5714285714285714 0.06345177664974619 0.625\n",
      "Loss is  1.1854513883590698\n",
      "1009\n",
      "accuracy, precision, recall 0.5630580357142857 0.07133917396745933 0.5816326530612245\n",
      "Loss is  1.257407546043396\n",
      "1033\n",
      "accuracy, precision, recall 0.5764508928571429 0.06985769728331177 0.574468085106383\n",
      "Loss is  1.5549975633621216\n",
      "1028\n",
      "accuracy, precision, recall 0.5736607142857143 0.07317073170731707 0.5757575757575758\n",
      "Loss is  1.3364852666854858\n",
      "1032\n",
      "accuracy, precision, recall 0.5758928571428571 0.0663265306122449 0.65\n",
      "Loss is  1.3706194162368774\n",
      "1057\n",
      "accuracy, precision, recall 0.58984375 0.06827309236947791 0.5666666666666667\n",
      "Loss is  1.4237868785858154\n",
      "1029\n",
      "accuracy, precision, recall 0.57421875 0.0682110682110682 0.5760869565217391\n",
      "Loss is  1.4451477527618408\n",
      "1053\n",
      "accuracy, precision, recall 0.5876116071428571 0.06827309236947791 0.5425531914893617\n",
      "Loss is  1.1351794004440308\n",
      "1070\n",
      "accuracy, precision, recall 0.5970982142857143 0.09308510638297872 0.6363636363636364\n",
      "Loss is  1.7223917245864868\n",
      "1041\n",
      "accuracy, precision, recall 0.5809151785714286 0.06422018348623854 0.5697674418604651\n",
      "Loss is  1.2192578315734863\n",
      "1061\n",
      "accuracy, precision, recall 0.5920758928571429 0.07863695937090433 0.6818181818181818\n",
      "Loss is  1.1825811862945557\n",
      "1055\n",
      "accuracy, precision, recall 0.5887276785714286 0.06631299734748011 0.6024096385542169\n",
      "Loss is  1.2002824544906616\n",
      "1082\n",
      "accuracy, precision, recall 0.6037946428571429 0.08635097493036212 0.5344827586206896\n",
      "Loss is  1.3273613452911377\n",
      "1076\n",
      "accuracy, precision, recall 0.6004464285714286 0.07723577235772358 0.6195652173913043\n",
      "Loss is  1.2827091217041016\n",
      "1068\n",
      "accuracy, precision, recall 0.5959821428571429 0.09018567639257294 0.6415094339622641\n",
      "Loss is  1.3592711687088013\n",
      "1064\n",
      "accuracy, precision, recall 0.59375 0.07307171853856563 0.5567010309278351\n",
      "Loss is  1.7288546562194824\n",
      "1072\n",
      "accuracy, precision, recall 0.5982142857142857 0.08502024291497975 0.6\n",
      "Loss is  1.2245010137557983\n",
      "1064\n",
      "accuracy, precision, recall 0.59375 0.07095046854082998 0.6091954022988506\n",
      "Loss is  1.3512543439865112\n",
      "1054\n",
      "accuracy, precision, recall 0.5881696428571429 0.06072874493927125 0.5172413793103449\n",
      "Loss is  1.297355055809021\n",
      "1048\n",
      "accuracy, precision, recall 0.5848214285714286 0.06515957446808511 0.5444444444444444\n",
      "Loss is  1.3890467882156372\n",
      "1054\n",
      "accuracy, precision, recall 0.5881696428571429 0.07227332457293036 0.632183908045977\n",
      "Loss is  1.319413185119629\n",
      "1047\n",
      "accuracy, precision, recall 0.5842633928571429 0.06324110671936758 0.5853658536585366\n",
      "Loss is  1.3550554513931274\n",
      "1044\n",
      "accuracy, precision, recall 0.5825892857142857 0.06892067620286085 0.6235294117647059\n",
      "Loss is  1.2158305644989014\n",
      "1063\n",
      "accuracy, precision, recall 0.5931919642857143 0.0758988015978695 0.6195652173913043\n",
      "Loss is  1.2232407331466675\n",
      "1074\n",
      "accuracy, precision, recall 0.5993303571428571 0.08310626702997276 0.5754716981132075\n",
      "Loss is  1.7454392910003662\n",
      "1017\n",
      "accuracy, precision, recall 0.5675223214285714 0.07142857142857142 0.5436893203883495\n",
      "Loss is  1.3390580415725708\n",
      "1031\n",
      "accuracy, precision, recall 0.5753348214285714 0.06768837803320563 0.6309523809523809\n",
      "Loss is  1.2009804248809814\n",
      "1021\n",
      "accuracy, precision, recall 0.5697544642857143 0.06666666666666667 0.5473684210526316\n",
      "Loss is  1.1632646322250366\n",
      "1031\n",
      "accuracy, precision, recall 0.5753348214285714 0.07049608355091384 0.5242718446601942\n",
      "Loss is  1.3230538368225098\n",
      "1010\n",
      "accuracy, precision, recall 0.5636160714285714 0.0617906683480454 0.5632183908045977\n",
      "Loss is  1.1910452842712402\n",
      "999\n",
      "accuracy, precision, recall 0.5574776785714286 0.05764411027568922 0.5287356321839081\n",
      "Loss is  1.1782878637313843\n",
      "1013\n",
      "accuracy, precision, recall 0.5652901785714286 0.06733167082294264 0.6352941176470588\n",
      "Loss is  1.2294323444366455\n",
      "1006\n",
      "accuracy, precision, recall 0.5613839285714286 0.06557377049180328 0.5360824742268041\n",
      "Loss is  1.3180185556411743\n",
      "1016\n",
      "accuracy, precision, recall 0.5669642857142857 0.07575757575757576 0.5769230769230769\n",
      "Loss is  1.2270047664642334\n",
      "1022\n",
      "accuracy, precision, recall 0.5703125 0.0810126582278481 0.5925925925925926\n",
      "Loss is  1.374523401260376\n",
      "1030\n",
      "accuracy, precision, recall 0.5747767857142857 0.05867014341590613 0.5294117647058824\n",
      "Loss is  1.1687383651733398\n",
      "1032\n",
      "accuracy, precision, recall 0.5758928571428571 0.07741116751269035 0.648936170212766\n",
      "Loss is  1.1938531398773193\n",
      "984\n",
      "accuracy, precision, recall 0.5491071428571429 0.06372549019607843 0.5416666666666666\n",
      "Loss is  1.3964680433273315\n",
      "1011\n",
      "accuracy, precision, recall 0.5641741071428571 0.06758448060075094 0.6\n",
      "Loss is  1.2261409759521484\n",
      "1009\n",
      "accuracy, precision, recall 0.5630580357142857 0.061635220125786164 0.5697674418604651\n",
      "Loss is  1.1241264343261719\n",
      "992\n",
      "accuracy, precision, recall 0.5535714285714286 0.06150061500615006 0.5747126436781609\n",
      "Loss is  1.3670945167541504\n",
      "976\n",
      "accuracy, precision, recall 0.5446428571428571 0.06234718826405868 0.51\n",
      "Loss is  1.3121092319488525\n",
      "1002\n",
      "accuracy, precision, recall 0.5591517857142857 0.06049382716049383 0.6282051282051282\n",
      "Loss is  1.100113868713379\n",
      "1016\n",
      "accuracy, precision, recall 0.5669642857142857 0.0583756345177665 0.575\n",
      "Loss is  1.2623718976974487\n",
      "1021\n",
      "accuracy, precision, recall 0.5697544642857143 0.08040201005025126 0.6213592233009708\n",
      "Loss is  1.3102225065231323\n",
      "1027\n",
      "accuracy, precision, recall 0.5731026785714286 0.061381074168797956 0.6075949367088608\n",
      "Loss is  1.3898438215255737\n",
      "1032\n",
      "accuracy, precision, recall 0.5758928571428571 0.07850707850707851 0.580952380952381\n",
      "Loss is  1.279340386390686\n",
      "999\n",
      "accuracy, precision, recall 0.5574776785714286 0.050505050505050504 0.49382716049382713\n",
      "Loss is  1.4611443281173706\n",
      "1013\n",
      "accuracy, precision, recall 0.5652901785714286 0.05947955390334572 0.7058823529411765\n",
      "Loss is  1.0911957025527954\n",
      "1046\n",
      "accuracy, precision, recall 0.5837053571428571 0.06455862977602109 0.5764705882352941\n",
      "Loss is  1.1656217575073242\n",
      "1016\n",
      "accuracy, precision, recall 0.5669642857142857 0.07124681933842239 0.5490196078431373\n",
      "Loss is  1.3370343446731567\n",
      "1010\n",
      "accuracy, precision, recall 0.5636160714285714 0.053735255570117955 0.40594059405940597\n",
      "Loss is  1.6194711923599243\n",
      "1055\n",
      "accuracy, precision, recall 0.5887276785714286 0.0678191489361702 0.5862068965517241\n",
      "Loss is  1.1216773986816406\n",
      "1044\n",
      "accuracy, precision, recall 0.5825892857142857 0.054376657824933686 0.5394736842105263\n",
      "Loss is  1.1975278854370117\n",
      "1051\n",
      "accuracy, precision, recall 0.5864955357142857 0.07313829787234043 0.5555555555555556\n",
      "Loss is  1.3282544612884521\n",
      "1035\n",
      "accuracy, precision, recall 0.5775669642857143 0.06388526727509779 0.5568181818181818\n",
      "Loss is  1.1687954664230347\n",
      "1045\n",
      "accuracy, precision, recall 0.5831473214285714 0.059219380888290714 0.4782608695652174\n",
      "Loss is  1.538482427597046\n",
      "1045\n",
      "accuracy, precision, recall 0.5831473214285714 0.06533333333333333 0.5157894736842106\n",
      "Loss is  1.234480857849121\n",
      "1048\n",
      "accuracy, precision, recall 0.5848214285714286 0.06481481481481481 0.5697674418604651\n",
      "Loss is  1.0810394287109375\n",
      "1056\n",
      "accuracy, precision, recall 0.5892857142857143 0.06147540983606557 0.4787234042553192\n",
      "Loss is  1.4547561407089233\n",
      "1025\n",
      "accuracy, precision, recall 0.5719866071428571 0.055916775032509754 0.5119047619047619\n",
      "Loss is  1.1446834802627563\n",
      "1019\n",
      "accuracy, precision, recall 0.5686383928571429 0.05844980940279543 0.5897435897435898\n",
      "Loss is  1.3758842945098877\n",
      "1049\n",
      "accuracy, precision, recall 0.5853794642857143 0.06241699867197875 0.5595238095238095\n",
      "Loss is  1.1433228254318237\n",
      "1072\n",
      "accuracy, precision, recall 0.5982142857142857 0.0703448275862069 0.5257731958762887\n",
      "Loss is  1.259691834449768\n",
      "1029\n",
      "accuracy, precision, recall 0.57421875 0.06666666666666667 0.51\n",
      "Loss is  1.3097896575927734\n",
      "1080\n",
      "accuracy, precision, recall 0.6026785714285714 0.07901907356948229 0.6170212765957447\n",
      "Loss is  1.2210655212402344\n",
      "1070\n",
      "accuracy, precision, recall 0.5970982142857143 0.06948228882833787 0.5666666666666667\n",
      "Loss is  1.2010993957519531\n",
      "1077\n",
      "accuracy, precision, recall 0.6010044642857143 0.05439330543933055 0.5131578947368421\n",
      "Loss is  1.3549636602401733\n",
      "1096\n",
      "accuracy, precision, recall 0.6116071428571429 0.07834757834757834 0.5288461538461539\n",
      "Loss is  1.2452785968780518\n",
      "1045\n",
      "accuracy, precision, recall 0.5831473214285714 0.06089309878213803 0.45918367346938777\n",
      "Loss is  1.1776431798934937\n",
      "1081\n",
      "accuracy, precision, recall 0.6032366071428571 0.0737134909596662 0.5408163265306123\n",
      "Loss is  1.2585886716842651\n",
      "1096\n",
      "accuracy, precision, recall 0.6116071428571429 0.08368200836820083 0.6060606060606061\n",
      "Loss is  1.2341454029083252\n",
      "1102\n",
      "accuracy, precision, recall 0.6149553571428571 0.06419400855920114 0.569620253164557\n",
      "Loss is  1.2802858352661133\n",
      "1078\n",
      "accuracy, precision, recall 0.6015625 0.07381615598885793 0.5196078431372549\n",
      "Loss is  1.3972413539886475\n",
      "1076\n",
      "accuracy, precision, recall 0.6004464285714286 0.07617728531855955 0.5288461538461539\n",
      "Loss is  1.3925163745880127\n",
      "1082\n",
      "accuracy, precision, recall 0.6037946428571429 0.06740027510316368 0.6049382716049383\n",
      "Loss is  1.208389163017273\n",
      "1072\n",
      "accuracy, precision, recall 0.5982142857142857 0.07565337001375516 0.5339805825242718\n",
      "Loss is  1.6123263835906982\n",
      "1077\n",
      "accuracy, precision, recall 0.6010044642857143 0.06740027510316368 0.5697674418604651\n",
      "Loss is  1.275938868522644\n",
      "1092\n",
      "accuracy, precision, recall 0.609375 0.08183079056865465 0.6082474226804123\n",
      "Loss is  1.265960454940796\n",
      "1085\n",
      "accuracy, precision, recall 0.60546875 0.07523939808481532 0.6395348837209303\n",
      "Loss is  1.6046068668365479\n",
      "1081\n",
      "accuracy, precision, recall 0.6032366071428571 0.06675938803894298 0.5454545454545454\n",
      "Loss is  1.2544885873794556\n",
      "1049\n",
      "accuracy, precision, recall 0.5853794642857143 0.05759162303664921 0.6567164179104478\n",
      "Loss is  1.0693687200546265\n",
      "1048\n",
      "accuracy, precision, recall 0.5848214285714286 0.07692307692307693 0.5471698113207547\n",
      "Loss is  1.8860965967178345\n",
      "1059\n",
      "accuracy, precision, recall 0.5909598214285714 0.07057256990679095 0.6022727272727273\n",
      "Loss is  1.1268401145935059\n",
      "1054\n",
      "accuracy, precision, recall 0.5881696428571429 0.07785234899328859 0.5321100917431193\n",
      "Loss is  1.6759284734725952\n",
      "1018\n",
      "accuracy, precision, recall 0.5680803571428571 0.07632600258732213 0.4957983193277311\n",
      "Loss is  1.5117018222808838\n",
      "1037\n",
      "accuracy, precision, recall 0.5786830357142857 0.056921086675291076 0.6285714285714286\n",
      "Loss is  1.1321463584899902\n",
      "1038\n",
      "accuracy, precision, recall 0.5792410714285714 0.07662835249042145 0.6593406593406593\n",
      "Loss is  1.1954025030136108\n",
      "993\n",
      "accuracy, precision, recall 0.5541294642857143 0.06699751861042183 0.5346534653465347\n",
      "Loss is  1.4087380170822144\n",
      "993\n",
      "accuracy, precision, recall 0.5541294642857143 0.05506883604505632 0.5\n",
      "Loss is  1.2835410833358765\n",
      "974\n",
      "accuracy, precision, recall 0.5435267857142857 0.06090133982947625 0.5154639175257731\n",
      "Loss is  1.4771736860275269\n",
      "984\n",
      "accuracy, precision, recall 0.5491071428571429 0.07515151515151515 0.5794392523364486\n",
      "Loss is  1.3534044027328491\n",
      "995\n",
      "accuracy, precision, recall 0.5552455357142857 0.06351183063511831 0.53125\n",
      "Loss is  1.2621910572052002\n",
      "985\n",
      "accuracy, precision, recall 0.5496651785714286 0.057177615571776155 0.5949367088607594\n",
      "Loss is  1.1249418258666992\n",
      "937\n",
      "accuracy, precision, recall 0.5228794642857143 0.05509964830011723 0.4895833333333333\n",
      "Loss is  1.303280234336853\n",
      "955\n",
      "accuracy, precision, recall 0.5329241071428571 0.06390532544378698 0.54\n",
      "Loss is  1.3735171556472778\n",
      "949\n",
      "accuracy, precision, recall 0.5295758928571429 0.06936416184971098 0.6122448979591837\n",
      "Loss is  1.2132753133773804\n",
      "973\n",
      "accuracy, precision, recall 0.54296875 0.06973995271867613 0.6483516483516484\n",
      "Loss is  1.2099244594573975\n",
      "920\n",
      "accuracy, precision, recall 0.5133928571428571 0.05561861520998865 0.550561797752809\n",
      "Loss is  1.1587834358215332\n",
      "961\n",
      "accuracy, precision, recall 0.5362723214285714 0.059788980070339975 0.6375\n",
      "Loss is  1.1501318216323853\n",
      "937\n",
      "accuracy, precision, recall 0.5228794642857143 0.07665903890160183 0.5826086956521739\n",
      "Loss is  1.2586313486099243\n",
      "924\n",
      "accuracy, precision, recall 0.515625 0.055304740406320545 0.6125\n",
      "Loss is  1.1754387617111206\n",
      "960\n",
      "accuracy, precision, recall 0.5357142857142857 0.07727797001153403 0.6767676767676768\n",
      "Loss is  1.3883836269378662\n",
      "953\n",
      "accuracy, precision, recall 0.5318080357142857 0.06451612903225806 0.6746987951807228\n",
      "Loss is  1.0939247608184814\n",
      "933\n",
      "accuracy, precision, recall 0.5206473214285714 0.07787810383747178 0.6216216216216216\n",
      "Loss is  1.3087577819824219\n",
      "959\n",
      "accuracy, precision, recall 0.53515625 0.06093189964157706 0.5204081632653061\n",
      "Loss is  1.2490202188491821\n",
      "956\n",
      "accuracy, precision, recall 0.5334821428571429 0.06478209658421673 0.5670103092783505\n",
      "Loss is  1.2065340280532837\n",
      "950\n",
      "accuracy, precision, recall 0.5301339285714286 0.05011655011655012 0.6142857142857143\n",
      "Loss is  1.2181141376495361\n",
      "961\n",
      "accuracy, precision, recall 0.5362723214285714 0.07134502923976609 0.6224489795918368\n",
      "Loss is  1.339744210243225\n",
      "936\n",
      "accuracy, precision, recall 0.5223214285714286 0.04074505238649592 0.5223880597014925\n",
      "Loss is  1.1375534534454346\n",
      "963\n",
      "accuracy, precision, recall 0.5373883928571429 0.06831566548881036 0.6041666666666666\n",
      "Loss is  1.5335110425949097\n",
      "947\n",
      "accuracy, precision, recall 0.5284598214285714 0.07019562715765247 0.6224489795918368\n",
      "Loss is  1.3456529378890991\n",
      "974\n",
      "accuracy, precision, recall 0.5435267857142857 0.06093189964157706 0.6144578313253012\n",
      "Loss is  1.254266619682312\n",
      "1010\n",
      "accuracy, precision, recall 0.5636160714285714 0.07950310559006211 0.6095238095238096\n",
      "Loss is  1.322630763053894\n",
      "993\n",
      "accuracy, precision, recall 0.5541294642857143 0.08643457382953182 0.6545454545454545\n",
      "Loss is  1.3044322729110718\n",
      "971\n",
      "accuracy, precision, recall 0.5418526785714286 0.06738868832731648 0.5490196078431373\n",
      "Loss is  1.2577341794967651\n",
      "996\n",
      "accuracy, precision, recall 0.5558035714285714 0.06335403726708075 0.5483870967741935\n",
      "Loss is  1.1616568565368652\n",
      "979\n",
      "accuracy, precision, recall 0.5463169642857143 0.06706586826347305 0.6222222222222222\n",
      "Loss is  1.187040090560913\n",
      "989\n",
      "accuracy, precision, recall 0.5518973214285714 0.05562422744128554 0.5357142857142857\n",
      "Loss is  1.394649624824524\n",
      "1004\n",
      "accuracy, precision, recall 0.5602678571428571 0.05721393034825871 0.6052631578947368\n",
      "Loss is  1.23075270652771\n",
      "984\n",
      "accuracy, precision, recall 0.5491071428571429 0.049079754601226995 0.547945205479452\n",
      "Loss is  1.1062238216400146\n",
      "1038\n",
      "accuracy, precision, recall 0.5792410714285714 0.0778061224489796 0.6630434782608695\n",
      "Loss is  1.3038527965545654\n",
      "1028\n",
      "accuracy, precision, recall 0.5736607142857143 0.06527415143603134 0.5102040816326531\n",
      "Loss is  1.511297583580017\n",
      "1051\n",
      "accuracy, precision, recall 0.5864955357142857 0.06982872200263504 0.6022727272727273\n",
      "Loss is  1.1992771625518799\n",
      "1035\n",
      "accuracy, precision, recall 0.5775669642857143 0.0792838874680307 0.6262626262626263\n",
      "Loss is  1.331565499305725\n",
      "1036\n",
      "accuracy, precision, recall 0.578125 0.057441253263707574 0.5641025641025641\n",
      "Loss is  1.1092703342437744\n",
      "1056\n",
      "accuracy, precision, recall 0.5892857142857143 0.06763925729442971 0.6071428571428571\n",
      "Loss is  1.167916178703308\n",
      "1035\n",
      "accuracy, precision, recall 0.5775669642857143 0.08527131782945736 0.5739130434782609\n",
      "Loss is  1.3663074970245361\n",
      "1052\n",
      "accuracy, precision, recall 0.5870535714285714 0.07076101468624833 0.5463917525773195\n",
      "Loss is  1.1333645582199097\n",
      "1054\n",
      "accuracy, precision, recall 0.5881696428571429 0.06391478029294274 0.5783132530120482\n",
      "Loss is  1.1571696996688843\n",
      "1054\n",
      "accuracy, precision, recall 0.5881696428571429 0.06809078771695594 0.5604395604395604\n",
      "Loss is  1.1080759763717651\n",
      "1055\n",
      "accuracy, precision, recall 0.5887276785714286 0.06896551724137931 0.5977011494252874\n",
      "Loss is  1.1750930547714233\n",
      "1085\n",
      "accuracy, precision, recall 0.60546875 0.07172413793103448 0.6046511627906976\n",
      "Loss is  1.1393101215362549\n",
      "1068\n",
      "accuracy, precision, recall 0.5959821428571429 0.06089309878213803 0.6\n",
      "Loss is  1.0606573820114136\n",
      "1078\n",
      "accuracy, precision, recall 0.6015625 0.08016304347826086 0.6145833333333334\n",
      "Loss is  1.325555443763733\n",
      "1087\n",
      "accuracy, precision, recall 0.6065848214285714 0.06392045454545454 0.4945054945054945\n",
      "Loss is  1.4753283262252808\n",
      "1068\n",
      "accuracy, precision, recall 0.5959821428571429 0.07095046854082998 0.6385542168674698\n",
      "Loss is  1.1468168497085571\n",
      "1117\n",
      "accuracy, precision, recall 0.6233258928571429 0.08985507246376812 0.5688073394495413\n",
      "Loss is  1.4163790941238403\n",
      "1121\n",
      "accuracy, precision, recall 0.6255580357142857 0.06921944035346098 0.5465116279069767\n",
      "Loss is  1.3882019519805908\n",
      "1095\n",
      "accuracy, precision, recall 0.6110491071428571 0.08050847457627118 0.5533980582524272\n",
      "Loss is  1.3132984638214111\n",
      "1115\n",
      "accuracy, precision, recall 0.6222098214285714 0.06569343065693431 0.5487804878048781\n",
      "Loss is  1.2634438276290894\n",
      "1115\n",
      "accuracy, precision, recall 0.6222098214285714 0.07758620689655173 0.6067415730337079\n",
      "Loss is  1.19524085521698\n",
      "1106\n",
      "accuracy, precision, recall 0.6171875 0.07659574468085106 0.6067415730337079\n",
      "Loss is  1.4026132822036743\n",
      "1102\n",
      "accuracy, precision, recall 0.6149553571428571 0.08169014084507042 0.6041666666666666\n",
      "Loss is  1.2601248025894165\n",
      "1080\n",
      "accuracy, precision, recall 0.6026785714285714 0.06179775280898876 0.5\n",
      "Loss is  1.2209067344665527\n",
      "1098\n",
      "accuracy, precision, recall 0.6127232142857143 0.07102272727272728 0.5555555555555556\n",
      "Loss is  1.187056303024292\n",
      "1104\n",
      "accuracy, precision, recall 0.6160714285714286 0.06704707560627675 0.5802469135802469\n",
      "Loss is  1.1344099044799805\n",
      "1062\n",
      "accuracy, precision, recall 0.5926339285714286 0.06830601092896176 0.5102040816326531\n",
      "Loss is  1.2323484420776367\n",
      "1112\n",
      "accuracy, precision, recall 0.6205357142857143 0.06896551724137931 0.6\n",
      "Loss is  1.2835657596588135\n",
      "1106\n",
      "accuracy, precision, recall 0.6171875 0.07823613086770982 0.5913978494623656\n",
      "Loss is  1.4696544408798218\n",
      "1102\n",
      "accuracy, precision, recall 0.6149553571428571 0.0951048951048951 0.6126126126126126\n",
      "Loss is  1.4845906496047974\n",
      "1063\n",
      "accuracy, precision, recall 0.5931919642857143 0.06085753803596127 0.46808510638297873\n",
      "Loss is  1.487668514251709\n",
      "1075\n",
      "accuracy, precision, recall 0.5998883928571429 0.07317073170731707 0.6206896551724138\n",
      "Loss is  1.253051519393921\n",
      "1092\n",
      "accuracy, precision, recall 0.609375 0.056818181818181816 0.5263157894736842\n",
      "Loss is  1.1159851551055908\n",
      "1095\n",
      "accuracy, precision, recall 0.6110491071428571 0.0702247191011236 0.5882352941176471\n",
      "Loss is  1.2005841732025146\n",
      "1084\n",
      "accuracy, precision, recall 0.6049107142857143 0.05923836389280677 0.5060240963855421\n",
      "Loss is  1.115416169166565\n",
      "1077\n",
      "accuracy, precision, recall 0.6010044642857143 0.07983761840324763 0.6276595744680851\n",
      "Loss is  1.3046748638153076\n",
      "1081\n",
      "accuracy, precision, recall 0.6032366071428571 0.07152682255845942 0.5909090909090909\n",
      "Loss is  1.1206613779067993\n",
      "1101\n",
      "accuracy, precision, recall 0.6143973214285714 0.07528409090909091 0.5698924731182796\n",
      "Loss is  1.3420933485031128\n",
      "1076\n",
      "accuracy, precision, recall 0.6004464285714286 0.06111111111111111 0.5238095238095238\n",
      "Loss is  1.1853723526000977\n",
      "1094\n",
      "accuracy, precision, recall 0.6104910714285714 0.0759493670886076 0.5684210526315789\n",
      "Loss is  1.3184070587158203\n",
      "1084\n",
      "accuracy, precision, recall 0.6049107142857143 0.07172995780590717 0.5151515151515151\n",
      "Loss is  1.2598910331726074\n",
      "1073\n",
      "accuracy, precision, recall 0.5987723214285714 0.06275579809004093 0.5897435897435898\n",
      "Loss is  1.3662426471710205\n",
      "1053\n",
      "accuracy, precision, recall 0.5876116071428571 0.08267716535433071 0.6116504854368932\n",
      "Loss is  1.2411761283874512\n",
      "1095\n",
      "accuracy, precision, recall 0.6110491071428571 0.09806629834254144 0.6173913043478261\n",
      "Loss is  1.2649294137954712\n",
      "1052\n",
      "accuracy, precision, recall 0.5870535714285714 0.06998654104979811 0.5148514851485149\n",
      "Loss is  1.412327527999878\n",
      "1065\n",
      "accuracy, precision, recall 0.5943080357142857 0.06784260515603799 0.5555555555555556\n",
      "Loss is  1.3980334997177124\n",
      "1069\n",
      "accuracy, precision, recall 0.5965401785714286 0.06504065040650407 0.5925925925925926\n",
      "Loss is  1.1134257316589355\n",
      "1030\n",
      "accuracy, precision, recall 0.5747767857142857 0.057104913678618856 0.45263157894736844\n",
      "Loss is  1.3680634498596191\n",
      "1072\n",
      "accuracy, precision, recall 0.5982142857142857 0.0739247311827957 0.6395348837209303\n",
      "Loss is  1.1155683994293213\n",
      "1055\n",
      "accuracy, precision, recall 0.5887276785714286 0.06382978723404255 0.5925925925925926\n",
      "Loss is  1.2120239734649658\n",
      "1039\n",
      "accuracy, precision, recall 0.5797991071428571 0.05614973262032086 0.47191011235955055\n",
      "Loss is  1.2841517925262451\n",
      "1071\n",
      "accuracy, precision, recall 0.59765625 0.0726552179656539 0.7432432432432432\n",
      "Loss is  1.1055123805999756\n",
      "1038\n",
      "accuracy, precision, recall 0.5792410714285714 0.07417218543046358 0.5045045045045045\n",
      "Loss is  1.303011417388916\n",
      "1036\n",
      "accuracy, precision, recall 0.578125 0.05695364238410596 0.4942528735632184\n",
      "Loss is  1.4375462532043457\n",
      "1071\n",
      "accuracy, precision, recall 0.59765625 0.06621621621621622 0.620253164556962\n",
      "Loss is  1.119292140007019\n",
      "1041\n",
      "accuracy, precision, recall 0.5809151785714286 0.06753246753246753 0.611764705882353\n",
      "Loss is  1.2861888408660889\n",
      "1068\n",
      "accuracy, precision, recall 0.5959821428571429 0.06693440428380187 0.6493506493506493\n",
      "Loss is  1.0639740228652954\n",
      "1062\n",
      "accuracy, precision, recall 0.5926339285714286 0.0739247311827957 0.5729166666666666\n",
      "Loss is  1.454330563545227\n",
      "1050\n",
      "accuracy, precision, recall 0.5859375 0.06016042780748663 0.5357142857142857\n",
      "Loss is  1.2058662176132202\n",
      "1072\n",
      "accuracy, precision, recall 0.5982142857142857 0.07671601615074024 0.6263736263736264\n",
      "Loss is  1.4996412992477417\n",
      "1048\n",
      "accuracy, precision, recall 0.5848214285714286 0.06550802139037433 0.5212765957446809\n",
      "Loss is  1.3968682289123535\n",
      "1048\n",
      "accuracy, precision, recall 0.5848214285714286 0.06442953020134229 0.5052631578947369\n",
      "Loss is  1.2456210851669312\n",
      "1086\n",
      "accuracy, precision, recall 0.6060267857142857 0.07724137931034483 0.6021505376344086\n",
      "Loss is  1.5079907178878784\n",
      "1072\n",
      "accuracy, precision, recall 0.5982142857142857 0.07506702412868632 0.6511627906976745\n",
      "Loss is  1.4349377155303955\n",
      "1023\n",
      "accuracy, precision, recall 0.5708705357142857 0.05111402359108781 0.4642857142857143\n",
      "Loss is  1.3096439838409424\n",
      "1033\n",
      "accuracy, precision, recall 0.5764508928571429 0.059602649006622516 0.4787234042553192\n",
      "Loss is  1.2251650094985962\n",
      "1032\n",
      "accuracy, precision, recall 0.5758928571428571 0.05179282868525897 0.4588235294117647\n",
      "Loss is  1.307459831237793\n",
      "1061\n",
      "accuracy, precision, recall 0.5920758928571429 0.06756756756756757 0.5494505494505495\n",
      "Loss is  1.233539342880249\n",
      "1040\n",
      "accuracy, precision, recall 0.5803571428571429 0.07402597402597402 0.59375\n",
      "Loss is  1.1617591381072998\n",
      "1054\n",
      "accuracy, precision, recall 0.5881696428571429 0.06317204301075269 0.5340909090909091\n",
      "Loss is  1.4056366682052612\n",
      "1069\n",
      "accuracy, precision, recall 0.5965401785714286 0.06334231805929919 0.6266666666666667\n",
      "Loss is  1.0428107976913452\n",
      "1047\n",
      "accuracy, precision, recall 0.5842633928571429 0.05170068027210884 0.4418604651162791\n",
      "Loss is  1.227242112159729\n",
      "1048\n",
      "accuracy, precision, recall 0.5848214285714286 0.06648575305291723 0.4666666666666667\n",
      "Loss is  1.3787938356399536\n",
      "1083\n",
      "accuracy, precision, recall 0.6043526785714286 0.07192254495159059 0.5777777777777777\n",
      "Loss is  1.4132630825042725\n",
      "1063\n",
      "accuracy, precision, recall 0.5931919642857143 0.08935611038107753 0.6538461538461539\n",
      "Loss is  1.3177045583724976\n",
      "1056\n",
      "accuracy, precision, recall 0.5892857142857143 0.07559681697612732 0.59375\n",
      "Loss is  1.304975986480713\n",
      "1051\n",
      "accuracy, precision, recall 0.5864955357142857 0.06989247311827956 0.5148514851485149\n",
      "Loss is  1.3396382331848145\n",
      "1049\n",
      "accuracy, precision, recall 0.5853794642857143 0.06374501992031872 0.5581395348837209\n",
      "Loss is  1.427457571029663\n",
      "1036\n",
      "accuracy, precision, recall 0.578125 0.05665349143610013 0.5180722891566265\n",
      "Loss is  1.1302437782287598\n",
      "1041\n",
      "accuracy, precision, recall 0.5809151785714286 0.07581699346405228 0.5686274509803921\n",
      "Loss is  1.4670279026031494\n",
      "1033\n",
      "accuracy, precision, recall 0.5764508928571429 0.05935483870967742 0.6052631578947368\n",
      "Loss is  1.0678013563156128\n",
      "1067\n",
      "accuracy, precision, recall 0.5954241071428571 0.08488063660477453 0.6464646464646465\n",
      "Loss is  1.2242993116378784\n",
      "1038\n",
      "accuracy, precision, recall 0.5792410714285714 0.07863695937090433 0.5405405405405406\n",
      "Loss is  1.3192179203033447\n",
      "1020\n",
      "accuracy, precision, recall 0.5691964285714286 0.07830551989730423 0.5304347826086957\n",
      "Loss is  1.3408869504928589\n",
      "1022\n",
      "accuracy, precision, recall 0.5703125 0.061224489795918366 0.5853658536585366\n",
      "Loss is  1.2422069311141968\n",
      "1019\n",
      "accuracy, precision, recall 0.5686383928571429 0.05384615384615385 0.5454545454545454\n",
      "Loss is  1.1830605268478394\n",
      "1033\n",
      "accuracy, precision, recall 0.5764508928571429 0.07653061224489796 0.631578947368421\n",
      "Loss is  1.1654717922210693\n",
      "1031\n",
      "accuracy, precision, recall 0.5753348214285714 0.08375634517766498 0.6285714285714286\n",
      "Loss is  1.2003321647644043\n"
     ]
    }
   ],
   "source": [
    "model_trained = train_model(model, criterion, optimizer, exp_lr_scheduler, num_epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
